An outline of a Fortran program will be introduced capable
of performing an MD simulation which will be extended as appropriate
to implement different parallel schemes for this dissertation.
%
An outline of the data to be gathered will also be presented here
along with the method by which it is gathered.

\section{Implementation}
\label{sec:methodology:subsec:implementation}

The simulation for this dissertation will be implemented in Fortran.
%
To allow for arbitrary parallel schemes to be implemented,
the logic of how an individual particle is stepped forward
in time and when and where that stepping should be applied
should be separated.
%
Similarly, the logic for finding the force between two particles
should be separated from the logic for combining all of these
\twobody{} forces to find the total instantaneous force on a particle.

To address this, a ``distribution class'' is implemented,
representing a distributed list of particles
along with a set of interfaces for
applying transformations over the particles.
%
These interfaces accept user defined functions which may specify
mappings over particles returning partial results,
predefined reduction operations and
mappings from fully reduced values back onto the particles.
%
The interfaces will then apply these mappings and reductions
as appropriate.
%
This allows for graceful handling of arbitrary user defined operations while
allowing the underlying parallel scheme to be changed as necessary.

%
%Q: What does an implementation look like?
This approach suggests a minimum number of operations to be implemented,
namely individual particle operations and particle list comparison
operations representing the integration and force update sections
of the MD simulation loop outlined in \FIG{fig:md_loop_flow_chart}.


\subsection{The \individualoperation{} Method}
\label{sec:the_individual_operation_method}

This method will accept a function defining a mapping from
particles to particles.
This will replace each particle in the list with it's mapped
particle.

An example serial implementation is in
\LST{lst:serial_individual_operation_implementation}.

\begin{lstlisting}[
    caption=Serial \individualoperation{} implementation.,
    label=lst:serial_individual_operation_implementation,
    gobble=4
]
    subroutine individual_operation(f)
        ...
        do i=1, num_particles
            particles(i) = f(particles(i), i)
        end do
    end subroutine individual_operation
\end{lstlisting}

This will be useful for initialising particle distributions
and performing integration steps.


\subsection{The \pairoperation{} Method}
\label{sec:the_pair_operation_method}

This method will accept a function defining a mapping from two
particles to a value, a predefined reduction operation, and
a mapping from a particle and a value a new particle.

An example serial implementation is in
\LST{lst:serial_pair_operation_implementation}.

\begin{lstlisting}[
    caption=Serial \pairoperation{} implementation.,
    label=lst:serial_pair_operation_implementation,
    gobble=4
]
    subroutine pair_operation(                     &
        pair_to_value, particle_value_to_particle, &
        reduction_type, reduction_init             &
    )
        ...

        do i=1, num_particles
            value = reduction_init

            do j=1, num_particles
                call pair_to_value(     &
                    particles(i), particles(j), &
                    tmp_value &
                )

                value = reduce(       &
                    value, tmp_value, &
                    reduction_type    & 
                )
            end do

            particles(i) = particle_value_to_particle( &
                particles(i), value
            )
        end do
    end subroutine pair_operation
\end{lstlisting}

As can be seen, some design choices have been taken which limit how
general the user defined operations can be.
%
There are also some unfortunate inconsistencies in the signatures of
user defined operations.
%
These are primarily due to language restrictions.
%
However, the minimum requirement is for the distribution to be able
to implement the \velocityverlet{} algorithm,
these problems are not a major concern.


\subsection{Errata and an Example}

Interfaces for printing information on the particles
out to disk and for performing some map-reduce operations over
the particles are also provided.
%
These are noted for completeness and are used for testing and debugging,
however they will not be used during benchmarking.

An example of an MD simulation using this model is provided in
\LST{lst:distribution_operation}.
%
Presented is an example of creating a distribution,
initialising the particles in the simulation,
an MD update loop and
printing the list of particles out to disk.


\begin{lstlisting}[
    caption=An example MD simulation,
    label=lst:distribution_operation,
]
program MD
    ...

    dist = new_distribution(...)

    call dist%individual_operation(init_particles)

    do while(time < end_time)
        ! Perform first half of velocity Verlet scheme
        call dist%individual_operation(VV_scheme_part_1)

        ! Find forces between particles due to
        ! LJ potential
        call dist%pair_operation(                      &
            map_force, unmap_force, dist%sum, zero_arr &
        )

        ! Perform second half of velocity Verlet scheme
        call dist%individual_operation(VV_scheme_part_2)

        ! Step time forward
        time = update_time(time)
    end do

    ! Output list of particles to screen
    call dist%print_particles(particle_to_string)

contains
    ! An overly simplistic initialisation scheme
    PURE function init_particles(particle_i, i)
        ...
        init_particles = Particle(        &
            pos=i, vel=0, mass=1, force=0 &
        )
    end function init_particles

    ! Find the force on particle1 due to particle2
    PURE subroutine map_force(        &
        particle_i, particle_j, force &
    )
        ...
        call find_LJ_force_between(        &
            particle_i, particle_j, force  &
        )
    end subroutine map_force

    ! Expect all the forces to have been summed together
    ! in the "total_force" parameter.
    ! Apply force to the particle and return it.
    PURE function unmap_force(particle_i, total_force)
        ...
        unmap_force = particle_i
        unmap_force%force = total_force
    end function unmap_force

    ! Accept a particle and return a string
    ! that can be printed.
    PURE subroutine particle_to_string( &
        particle_i, i, output_string     &
    )
        ...
        write(output_string, *) particle_i%pos
    end subroutine particle_to_string
end program MD
\end{lstlisting}



\section{Performance Measurement}

%
%Q: What should we be measuring?
In a parallel MD simulation, the time for a single step is strongly dependent
on both the number of particles present and
the number of parallel processes the simulation is running on.
%
This suggests a 2D space over which the performance of the implementation
might be evaluated.

%
%Q: What will we measure?
As successful MD simulations can be carried out on well defined
systems sizes, evaluating just how large a simulation can be run
is not particularly interesting.
%
As such, several fixed system sizes will be tested, namely
$N = 2^{9} = 512$,
$N = 2^{12} = 4096$ and
$N = 2^{15} = 32768$
particles.
%
These are chosen mainly so effects can be studied where the number of cores
used is small compared to the system, large compared to the system
and comparable to the system.
%
In particular, system sizes were chosen where communication times
tended to surpass calculation times within $P = 32768$ cores
being used.
%
The minimum system size was chosen to reflect a number of cores
exceeding the number of cores on a single \hector{} node and yet well
below the total number of cores on the machine.
%
The maximum system size was chosen both for time to completion for
a single repetition, and for it's large size compared to the number of
cores on a \hector{} node.

%
%Q: How will we measure these?
A very simple model of MD is used to distil the fundamental calculation
from the implementation.
%
Specifically, the simulation will perform a loop consisting of two
individual operations over the particles representing the first
and second halves of the \velocityverlet{} integration scheme and one
pair operation representing the force evaluation section of the scheme.
%
The loop will be run several times to extract some statistical significance
from the timings.
%
In particular, loops will be repeated until at least 5 iterations
have been performed and the overall time is comparable to 1 second.

%
%Q: Will we measure IO?
We will perform no I/O within the simulation.
%
While this is not representative of a real MD code, and indeed I/O
is itself an important factor in the performance of an MD simulation,
it is left outside the scope of this discussion.
%
I/O is implemented primarily for testing and debugging purposes.

%
%Q: How accurate will the benchmarked simulations be?
There is no pretence made of performing accurate simulations in
the benchmarks performed,
only that the operation performs the same calculations and
communications as an MD simulation.
%
The entire step of allowing the system to come to
thermal equilibrium will therefore be ignored.
%
This is primarily to reduce the time required to perform a benchmark.
%
This is not expected to have a significant impact on performance metrics
as no particle domain specific distributions are dealt with here,
so the particles having improper positions or velocities is of
little consequence to the number of operations and communications
performed on each process.
