We will outline our initial implementations of
the replicated data and systolic loop MD codes
along with our methods for extracting performance data from our codes.

\subsection{Implementation}

The code is implemented in Fortran.
%
In an attempt to decouple our simulation from the underlying code,
we try to abstract our underlying
particle distribution,
particle comparison schemes and
data gathering schemes
from our numerical integrator and pair force finding scheme.
%
To allow for arbitrary distribution schemes to be implemented,
we must ensure that potential redundant calculations and reductions
across processes may be easily implemented.

To this end, we implement a distribution class
representing a distributed list of particles
along with a set of interfaces for applying transformations over the particles.
%
These interfaces accept user defined functions which provide
a mapping over particles returning partial results,
predefined reduction operators and
a mapping from fully reduced values back onto the particles.
%
This allows us to gracefully handle arbitrary user defined operations while
leaving us free to change our underlying distribution and operations
at will.

%
%Q: What does an implementation look like?
Our approach suggests a minimum number of operations we must implement,
namely individual particle operations and particle list comparison
operations.
%
\begin{description}[style=nextline]
\item[individual\_operation]
    This method will accept a function defining a mapping from
    particles to particles.
    This will replace each particle in the list with it's mapped
    particle.

    An example serial implementation is in
    \LST{lst:serial_individual_operation_implementation}.

    \begin{lstlisting}[
        caption=Serial individual\_operation implementation.,
        label=lst:serial_individual_operation_implementation,
        gobble=8
    ]
        subroutine individual_operation(f)
            ...
            do i=1, num_particles
                particles(i) = f(particles(i), i)
            end do
        end subroutine individual_operation
    \end{lstlisting}
    
    This will be useful for initialising particle distributions
    and performing integration steps.

\item[pair\_operation]
    This method will accept a function defining a mapping from two
    particles to a value, a predefined reduction operation, and
    a mapping from a particle and a value a new particle.

    An example serial implementation is in
    \LST{lst:serial_pair_operation_implementation}.

    \begin{lstlisting}[
        caption=Serial pair\_operation implementation.,
        label=lst:serial_pair_operation_implementation,
        gobble=8
    ]
        subroutine pair_operation(                     &
            pair_to_value, particle_value_to_particle, &
            reduction_type                             &
        )
            ...
            do i=1, num_particles
                value = reduction_identity(reduction_type)

                do j=1, num_particles
                    tmp_value = pair_to_value(     &
                        particles(i), particles(j) &
                    )

                    value = reduce(       &
                        value, tmp_value, &
                        reduction_type    & 
                    )
                end do

                particles(i) = particle_value_to_particle( &
                    particles(i), value
                )
            end do
        end subroutine pair_operation
    \end{lstlisting}
\end  {description}

As can be seen, we have taken several design choices which limit how
general the user defined operations can be.
%
This is primarily due to language restrictions.
%
However, as we are focusing primarily on implementing
the \velocityverlet{} algorithm, this is not a major concern for us.

We further provide an interface for printing information on the particles
out to disk.
%
We are not particularly concerned with IO efficiency in this dissertation,
so we simply print out to the screen.
%
This is noted for completeness, and implementations for specific distributions
will not be documented.

An example of an MD simulation using this model is provided in
\LST{lst:distribution_operation}.

\begin{lstlisting}[
    caption=An example MD simulation,
    label=lst:distribution_operation,
]
program MD
    ...

    dist = new_distribution(...)

    call dist%individual_operation(init_particles)

    do while(time < end_time)
        ! Perform first half of velocity Verlet scheme
        call dist%individual_operation(VV_scheme_part_1)

        ! Find forces between particles due to
        ! LJ potential
        call dist%pair_operation(            &
            map_force, unmap_force, dist%sum &
        )

        ! Perform second half of velocity Verlet scheme
        call dist%individual_operation(VV_scheme_part_2)

        ! Step time forward
        time = update_time(time)
    end do

    ! Output list of particles to screen
    call dist%print_particles(particle_to_string)

contains
    ! An overly simplistic initialisation scheme
    PURE function init_particles(particle_i, i)
        ...
        init_particles = Particle(        &
            pos=i, vel=0, mass=1, force=0 &
        )
    end function init_particles

    ! Find the force on particle1 due to particle2
    PURE function map_force(particle_i, particle_j)
        ...
        map_force = find_LJ_force_between( &
            particle_i, particle_j           &
        )
    end function map_force

    ! Expect all the forces to have been summed together
    ! in the "total_force" parameter.
    ! Apply force to the particle and return it.
    PURE function unmap_force(particle_i, total_force)
        ...
        unmap_force = particle_i
        unmap_force%force = total_force
    end function unmap_force

    ! Accept a particle and return a string
    ! that can be printed.
    PURE subroutine particle_to_string( &
        particle_i, i, output_string     &
    )
        ...
        write(output_string, *) particle_i%pos
    end subroutine particle_to_string
end program MD
\end{lstlisting}



\subsection{Performance Measurement}

%
%Q: What should we be measuring?
In a parallel MD simulation, the time for a single step is strongly dependent
on both the number of particles present and
the number of parallel processes the simulation is running on.
%
This suggests a 2D space over which we might evaluate performance.

%
%Q: What will we measure?
As successful MD simulations can be carried out on already well defined
systems, we are not so interested in evaluating just how large we can
make our simulations.
%
As such, we choose several fixed system sizes to test, namely
$2^{9} = 512$, $2^{12} = 4096$, $2^{15} = 32768$ particles.
%
These are chosen mainly so we can study effects when the number of cores
used are small compared to the system, large compared to the system
and comparable to the system.
%
The minimum system size was chosen to reflect a number of cores
exceeding the number of cores on a single \hector{} node and yet well
below the total number of cores on the machine.
%
The maximum system size was chosen both for time to completion for
a single repetition, and for it's large size compared to the number of
cores on a \hector{} node.

%
%Q: How will we measure these?
We use a very simple model of MD to distil the fundamental calculation
with our implementation.
%
Specifically, our simulation will perform a loop consisting of two
individual operations over our particles representing the first
and second halves of the \velocityverlet{} integration scheme and one
pair operation representing the force evaluation section of the scheme.
%
The loop will be run several times to extract some statistical significance
from our timings.
%
In particular, loops will be repeated until at least 5 iterations
have been performed and the overall time is comparable to 1 second.

%
%Q: Will we measure IO?
We will perform no I/O within the simulation.
%
While this is not representative of a real MD code, and indeed I/O
is itself an important factor in the performance of an MD simulation,
we leave it outside the scope of this discussion.
%
We implement I/O primarily for testing purposes.

%
%Q: How accurate will the benchmarked simulations be?
We make no pretence of performing accurate simulations in our benchmarks,
so the entire step of allowing the system to come to
thermal equilibrium will be ignored.
%
This is primarily to reduce the time required to perform a benchmark.
%
This is not expected to have a significant impact on performance metrics
as we are not dealing with any particle domain specific distributions,
so the particles having improper positions or velocities is of
little consequence to the number of operations and communications
performed.
%
It will, however, be considered when testing our software so we are
certain our code is at least doing the work of a realistic simulation.

