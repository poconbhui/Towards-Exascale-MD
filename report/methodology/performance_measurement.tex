\section{Performance Measurement}

%
%Q: What should we be measuring?
In a parallel MD simulation, the time for a single step is strongly dependent
on both the number of particles present and
the number of parallel processes the simulation is running on.
%
This suggests a 2D space over which the performance of the implementation
might be evaluated.

%
%Q: What will we measure?
As successful MD simulations can be carried out on well defined
systems sizes, interesting results can be derived with relatively
small systems.
%
As such, several fixed system sizes will be tested, namely
$N = 2^{9} = 512$,
$N = 2^{12} = 4096$ and
$N = 2^{15} = 32768$
particles.
%
These are chosen mainly so effects can be studied where the number of cores
used is small compared to the system, large compared to the system
and comparable to the system.

The system sizes were also chosen as communication times
tended to surpass calculation times within $P = 32768$ cores
being used.
%
The minimum system size was chosen to reflect a number of particles
exceeding the number of cores on a single \hector{} node and yet far
below the total number of cores on the machine.
%
The maximum system size was chosen both for time to completion for
a single repetition, and for its size being comparable to the number of
cores on \hector{}.

%
%Q: How will we measure these?
A very simple model of MD is used to distil the fundamental calculation
from the implementation.
%
Specifically, the simulation will perform a loop consisting of two
individual operations over the particles representing the first
and second halves of the \velocityverlet{} integration scheme and one
pair operation representing the force evaluation section of the scheme.
%
The loop will be run several times to extract some statistical significance
from the timings.
%
In particular, loops will be required to perform at least 5 iterations
and have a run time comparable to 1 second.

%
%Q: Will we measure IO?
No I/O will be performed during a benchmark.
%
While this is not representative of a real MD code, and indeed I/O
is itself an important factor in the performance of an MD simulation,
it is left outside the scope of this discussion.
%
I/O is implemented primarily for testing and debugging purposes.

%
%Q: How accurate will the benchmarked simulations be?
As the implementations use particle list decomposition rather
than domain decomposition, the placement of particles on
processes is not tied to the simulation.
%
This means, to accurately measure the run time of the system,
it is not necessary to perform a physically accurate simulation.
%
As such, each method may be run separately.
%
As the \individualoperation{} and \pairoperation{} methods will
be tested separately, the MD simulation steps outlined in
\FIG{fig:phases_of_md_simulation}
will not be followed in full.
%
In particular, the initialisation and equilibriation steps will
be skipped.
%
The particle distribution may be initialised with arbitrary data.


\subsection{Benchmarking Plan}

Benchmarks will be performed on \hector{} Phase 3 using
32 cores per node with 32~GB of memory available to each node.
%
Each benchmark will be performed on the range of $[1,32768]$ cores
in powers of 2.
%
That is, benchmarks will be performed on
$1$, $2$, $4$, $8$, $\dots{}$, $32768$ cores.
%
Measurements will be made of the time taken for $n$ repetitions of a given
method where $n \ge{} 5$ and the time $t \sim{} 1$~second.
%
This is performed by timing 5 repetitions of the code.
%
If this time is less than 0.1~seconds, the repetition count is
multiplied by ten, and the code timed again.
%
This is repeated until until the time measured is greater than 0.1~seconds.
%
These measurements will be taken several times, at different times of
day and the lowest recorded time used here.

Measurements for implementations of
the \individualoperation{} and \pairoperation{} method
for each parallelisation scheme will be made.
%
These will be measured in the cases where both MPI and calculations
are performed, measuring the total execution time of the method;
the case where MPI communications are skipped and calculations are
performed, measuring the calculation time of the method; and
the case where calculations are skipped and MPI communications are
performed, measuring the communication time of the method.

Four distributions will be benchmarked:
%
the replicated distribution; the systolic distribution;
the shared and replicated distribution; and
the replicated systolic distribution.

Each implementation is left relatively unoptimised to ensure
the clarity of the implementation although
some thought has been put into the code to avoid inappropriately slow code.
%
These are compiled using crayftn and mpich2, using the ftn compiler with
no flags specified on the command line, beyond that needed to compile
the code.
%
MPI routines are implemented using blocking calls to produce the worst
case communication time.

\begin{table}
    \begin{tabular}{|l|l|}
        \hline
        Machine & \hector{} Phase 3 \\
        Cores Per Node & 32 \\
        Memory Per Node & 32~GB \\
        \hline
        Schemes Tested & \replicateddata{}, \systolicloop{}, \\
                       & \sharedandreplicateddata{},
                         \replicatedsystolicloop{} \\
        Methods Tested & \individualoperation{}, \pairoperation{} \\
        Regimes Tested & Total Time, MPI Only, Calculation Only \\
        Core Range Tested & 1-32768 \\
        Minimum Measurements & 5 repetitions and 0.1 seconds \\
        \hline
    \end{tabular}
    \caption{
        Summary of benchmarks to be performed and the hardware and
        configuration they will be performed on.
    }
    \label{table:benchmark_configuration}
\end{table}
