\subsection{Systolic Loop}

The systolic loop scheme is initialised by allocating 3 arrays of
size $N/P$ on every process.
%
The system is then split roughly evenly across all the processes
and is held, updated and shared using these three lists.


%
% Systolic individual_operation v0
%

\subsubsection{Implementation of the \individualoperation{} Method}
This is implemented by having each process update its local list
of particles.

This should take $\bigO{N/P}$ time.

\begin{figure}[!h]
    \input{parallel_implementation/v0/systolic.individual_operation.512.logtime.plt}
    \caption{\vZeroTimeCaption{Systolic Loop}{\individualoperation{}}{512}}
    \label{fig:v0_systolic_individual_operation_512_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{parallel_implementation/v0/systolic.individual_operation.4096.logtime.plt}
    \caption{\vZeroTimeCaption{Systolic Loop}{\individualoperation{}}{4096}}
    \label{fig:v0_systolic_individual_operation_4096_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{parallel_implementation/v0/systolic.individual_operation.32768.logtime.plt}
    \caption{\vZeroTimeCaption{Systolic Loop}{\individualoperation{}}{32768}}
    \label{fig:v0_systolic_individual_operation_32768_logtime}
\end  {figure}

As seen in
\FIG{fig:v0_systolic_individual_operation_512_logtime},
\FIG{fig:v0_systolic_individual_operation_4096_logtime} and
\FIG{fig:v0_systolic_individual_operation_32768_logtime}
the current implementation satisfies this.

There appear to be a few unexpected data points in the mpi only timings,
but these are unlikely to be anything except an error.
%
As these are approaching nanosecond execution times, it is
unsurprising that they may pick up unexpected errors as the system clock
has at most a nanosecond resolution time.
%
Given the processor operates on about this time, it is also unsurprising
if some odd times may be picked up from taking measurements so
close to this time scale.


%
% Systolic pair_operation v0
%

\subsubsection{Implementation of the \pairoperation{} Method}
This is implemented by having each process use three lists of particles,
each of size $P/N$.

The first list is the processes local list of particles.

The second list will be referred to as the foreign list, and
represents a list originating from another process.

The third list is a swap list to allow a process to receive a new
foreign list list from the right
while also sending its old foreign list to the left
during a systolic pulse.

Initially, a process will copy its local list to the foreign list
and perform a partial force update on its local list using this
foreign list.
%
The system will then perform a systolic pulse.
After a systolic pulse, every foreign list should move one process
to the left in the systolic loop.
%
This is performed by copying the old foreign list into the
swap list, and using an MPI\_sendrecv to send the swap list to
the left process while receiving from
the right process into the foreign list.
%
When a new foreign list is received, another partial force update
is performed on the local list.
%
This process is repeated $P-1$ times.

Each list comparison between systolic pulses should take $\bigO{(N/P)^2}$ time.
%
For a given timestep, there should be $P$ of these list comparisons
performed, giving an over calculation time of $\bigO{N^2/P}$.

Given each pulse should be passing $N/P$ particles between two processes,
we expect this to take $\bigO{N/P + l}$ time where $l$ is a constant latency.
%
With $P$ pulses on each time step, we expect a communication time of
$\bigO{(N/P + l)P}$.

Combining our calculation and communication terms, the systolic loop approach
should run in $\bigO{N^2/P + dN + dlP}$ time
where $N$ is the number of particles in the system,
$P$ is the number of processes used and
$l$ is a constant latency and
$d$ is a constant.

\begin{figure}[!h]
    \input{parallel_implementation/v0/systolic.pair_operation.512.logtime.plt}
    \caption{\vZeroTimeCaption{Systolic Loop}{\pairoperation{}}{512}}
    \label{fig:v0_systolic_pair_operation_512_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{parallel_implementation/v0/systolic.pair_operation.4096.logtime.plt}
    \caption{\vZeroTimeCaption{Systolic Loop}{\pairoperation{}}{4096}}
    \label{fig:v0_systolic_pair_operation_4096_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{parallel_implementation/v0/systolic.pair_operation.32768.logtime.plt}
    \caption{\vZeroTimeCaption{Systolic Loop}{\pairoperation{}}{32768}}
    \label{fig:v0_systolic_pair_operation_32768_logtime}
\end  {figure}

We see in 
\FIG{fig:v0_systolic_pair_operation_512_logtime},
\FIG{fig:v0_systolic_pair_operation_4096_logtime} and
\FIG{fig:v0_systolic_pair_operation_32768_logtime}
that, much like in the replicated case, that the system scales
roughly as $N/P$ when $P \ll{} N$.

With a communication term scaling as $P$, we see the point in which
communications dominate comes much sooner.
%
However, we also note that it appears when $P \approx{} N$.
%
Taking our previous approach of finding an optimum $k$ for $P = N/k$,
we find $k \approx{} 8$.

This appears to hold for our three system sizes, although there does appear
to be an unexpected jump in
\FIG{fig:v0_systolic_pair_operation_32768_logtime}
between 512 and 1024 processes.
%
It is unclear whether this is a genuine effect, or simply an error in
measurement.

Ignoring the unexpected jump in 
\FIG{fig:v0_systolic_pair_operation_32768_logtime},
we may conclude that the minimum time to completion for out system
scales linearly with the number of particles.

It would appear the implimentation ultimately becomes slowed due to
communication latency, and in particular, due to a large number
of these communications.
%
The initial scaling of this term is due to the number of particles
in the system, however,
scaling with $P$ is a result of having $P$ communications
per time step.
%
Looking at our derivation for communication times,
we conclude that this must be an effect of latency.
