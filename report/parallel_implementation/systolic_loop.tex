\subsection{Systolic Loop}

%
%Q: What data does the Systolic Loop implementation use?
The \systolicloop{} scheme, as described in
\SEC{sec:background:subsec:systolic_loop},
is initialised by allocating 3 arrays of particles of
size $N/P$ on every process for storing and transferring particles
along with an array of double precision values of size $3*N/P$ for
partial results for reduction operations.
%
The full system of particles is then
split roughly evenly across all the processes
and is held, updated and shared using these three lists.

In this section, the implementation details and performance of
of the \individualoperation{} and \pairoperation{} methods
for the \systolicloop{} scheme using MPI will be analysed and discussed.


%
% Systolic individual_operation v0
%

\subsubsection{Implementation of the \individualoperation{} Method}

The \individualoperation{} method, as outlined in
\SEC{sec:the_individual_operation_method}
for the \systolicloop{} scheme
is implemented by having each process update its local list of particles.


%
% Overall speedup plot
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.individual_operation.logspeedup.plt%
    }
    \caption{
        \vZeroSpeedupCaption
            {\systolicloop{}}
            {\individualoperation{}}
    }
    \label{fig:v0_systolic_individual_operation_speedups}
\end{figure}


\vZeroSpeedupExplanation
    {\FIG{fig:v0_systolic_individual_operation_speedups}}
    {\systolicloop{}}
    {\individualoperation{}}


%
%Q: What is the speedup of the Systolic Loop individual_operation method?

\FIG{fig:v0_systolic_individual_operation_speedups} quite good speedup
up to $P = N$.
%
It is interesting that the speedup is not perfect, given there are
no communications performed in this method.

%
%Q: What is the expected calculation time of the Systolic Loop individual_operation method?
As each update is an $\bigO{1}$ operation and there are $N/P$ particles
to update,
the calculations should take a time
\begin{equation}
    \frac{N}{P}\bigO{1} = \bigO{\frac{N}{P}}
\end  {equation}
%
%Q: What is the expected communication time of the Systolic Loop individual_operation method?
As there are no MPI communications performed, the communications should
take a time $\bigO{1}$, to allow for function calls and branch evaluations.
%
%Q: What is the expected overall time of the Systolic Loop individual_operation method?
The overall time should therefore be
\begin{equation}
    \bigO{\frac{N}{P}} + \bigO{1} = \bigO{\frac{N}{P}}
\end{equation}


%
% Individual breakdowns
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.individual_operation.512.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\systolicloop{}}
            {\individualoperation{}}
            {512}
    }
    \label{fig:v0_systolic_individual_operation_512_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.individual_operation.4096.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\systolicloop{}}
            {\individualoperation{}}
            {4096}
    }
    \label{fig:v0_systolic_individual_operation_4096_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.individual_operation.32768.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\systolicloop{}}
            {\individualoperation{}}
            {32768}
    }
    \label{fig:v0_systolic_individual_operation_32768_logtime}
\end  {figure}

\vZeroTimeExplanation
    {\FIG{fig:v0_systolic_individual_operation_512_logtime}}
    {\FIG{fig:v0_systolic_individual_operation_4096_logtime}}
    {\FIG{fig:v0_systolic_individual_operation_32768_logtime}}
    {\individualoperation{}}
    {\systolicloop{}}


%
%Q: Where and why does scaling stop for the Systolic Loop individual_operation method?
\FIG{fig:v0_systolic_individual_operation_512_logtime},
\FIG{fig:v0_systolic_individual_operation_4096_logtime} and
\FIG{fig:v0_systolic_individual_operation_32768_logtime}
appear to follow $\bigO{N/P}$ scaling.
%
Where $P \sim{} N$, the scaling drops off slightly.
%
This could be due to the very small amount of work performed
inside the function call, where function call and loop overheads
become comparable to the calculation time.
%
This is supported by the scaling of each graph tending to drop off
as execution times reach $\sim{} 10^{-7}$~s.

The MPI only scaling times here are unexpectedly nonzero.
%
As turning calculations or MPI off in the code is implemented in
the form of global flags and conditional branches, this timing
could be a result of having to perform a function call and
evaluate a flag.
%
As previously suggested, this could have the potential to impact on
operations occuring at scales of $10^{-7}$~s, so it is plausible
that these effects may occur around $10^-{8}$~s for a much simplified
function body.


%
% Systolic pair_operation v0
%

\subsubsection{Implementation of the \pairoperation{} Method}

The \pairoperation{} method, as outlined in 
\SEC{sec:the_pair_operation_method}
is implemented by arranging the processes in a ring and passing
packets of particles around the ring while the processes perform
partial updates to their local particles using these packages.


%
%Q: What is the speedup of the Systolic Loop pair_operation method?

%
% Overall speedup plot
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.pair_operation.logspeedup.plt%
    }
    \caption{
        \vZeroSpeedupCaption
            {\systolicloop{}}
            {\pairoperation{}}
    }
    \label{fig:v0_systolic_pair_operation_speedups}
\end{figure}


\vZeroSpeedupExplanation
    {\FIG{fig:v0_systolic_pair_operation_speedups}}
    {\systolicloop{}}
    {\pairoperation{}}

The speedup for each system size in 
\FIG{fig:v0_systolic_pair_operation_speedups}
appears to level off rapidly around $P = N/8$.
%
Until that point, speedup appears to be almost perfectly linear.
%
The point where the speedup graphs begin tipping over appears to
scale roughly with the number of particles in the system,
although not quite perfectly.


The \pairoperation{} method of the \systolicloop{} scheme is
implemented by having each process use three lists of particles,
each of size $N/P$.
%
The first list is the processes local list of particles.
%
The second list is used to receive a list of particles originating from
another process.
%
The third list is used to send a list of particles to another process.

When a systolic pulse is performed,
each process will copy its current ``receive''
list into its ``send'' list and then perform an \mpisendrecv{}, receiving
into the receive list from the ``left'' process and sending the send list
to the ``right'' process.
%
On each pulse, the process will compare the $N/P$ particles in its
local list to the $N/P$ particles it has just received and
add partial updates to a ``partial results'' array.

%
%Q: What is the expected calculation time of the Systolic Loop pair_operation method?
To perform all the partial updates for a one local particle
with a given receive list using an $\bigO{1}$ operation should take
\begin{equation}
    \frac{N}{P}\bigO{1} = \bigO{\frac{N}{P}}
\end  {equation}
%
Performing $N/P$ partial updates should take a time
\begin{equation}
    \frac{N}{P}\bigO{\frac{N}{P}} = \bigO{\left( \frac{N}{P} \right)^2}
\end  {equation}
%
As there are $P$ systolic pulses performed per time step,
there should be $P$ such partial updates
performed, giving an overall calculation time of
\begin{equation}
    \label{eqn:systolic_loop_pair_operation_overall_time}
    P\bigO{ \left(\frac{N}{P}\right)^2 } = \bigO{\frac{N^2}{P}}
\end  {equation}


For this particular implementation,
this method is initialised by having each process
copy its local list to its receive list.
%
Each process will then perform a partial update using this receive
list, and then perform a systolic pulse.
%
When a new foreign list is received, another partial force update
is performed on the local list.
%
The system will perform $P-1$ systolic pulses overall, at which point
each process should have received a list of particles originating
from each other process exactly once.
%
This doesn't effect the result of
\EQN{eqn:systolic_loop_pair_operation_overall_time}.


%
%Q: What is the expected communication time of the Systolic Loop pair_operation method?
Given each pulse should be passing $N/P$ particles between two processes
with a fixed latency $l$, each pulse is expected to take a time
$\bigO{N/P + l}$.
%
With $P-1$ pulses on each time step, the communication per timestep should be
\begin{equation}
    \begin{split}
        (P-1)\bigO{N/P + l}
            &= \bigO{(N/P + l)(P-1)} \\
            &= \bigO{(N + lP - N/P - l} \\
            &\approx{} \bigO{N + lP} \\
            &\approx{} \bigO{N + P}
    \end{split}
\end{equation}

%
%Q: What is the expected overall time of the Systolic Loop pair_operation method?
Combining the calculation and communication terms, the \systolicloop{} approach
should run in a time
\begin{equation}
    \begin{split}
        \bigO{\frac{N^2}{P}} + \bigO{N + P - N/P}
            &\approx{} \bigO{\frac{N^2}{P} + N + P - N/P} \\
            &= \bigO{\frac{N^2}{P} + P}
    \end{split}
\end{equation}
assuming $P \ll{} N^2$, which for this implementation will hold true.


%
% Individual breakdowns
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.pair_operation.512.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\systolicloop{}}
            {\pairoperation{}}
            {512}
    }
    \label{fig:v0_systolic_pair_operation_512_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.pair_operation.4096.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
        {\systolicloop{}}
        {\pairoperation{}}
        {4096}
    }
    \label{fig:v0_systolic_pair_operation_4096_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        systolic.pair_operation.32768.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\systolicloop{}}
            {\pairoperation{}}
            {32768}
    }
    \label{fig:v0_systolic_pair_operation_32768_logtime}
\end  {figure}

\vZeroTimeExplanation
{\FIG{fig:v0_systolic_pair_operation_512_logtime}}
{\FIG{fig:v0_systolic_pair_operation_4096_logtime}}
{\FIG{fig:v0_systolic_pair_operation_32768_logtime}}
{\pairoperation{}}
{\systolicloop{}}

%
%Q: Where and why does scaling stop for the Systolic Loop pair_operation method?
From
\FIG{fig:v0_systolic_pair_operation_512_logtime},
\FIG{fig:v0_systolic_pair_operation_4096_logtime} and
\FIG{fig:v0_systolic_pair_operation_32768_logtime}
it is clear that the system scales as $N/P$ when $P \ll{} N$.
%
However, it begins deviating significantly from linear scaling
when $N/P \approx{} 32$.
%
With a communication term scaling as $P$, and a calculation term
scaling as $N/P$, the communications term dominates far before $P = N$.
%
In each case, connunications approach close to pass calculations in time
when $N/P = 8$.

The implimentation ultimately becomes slowed due to
communication latency caused by a large number
of systolic pulses per time step.
%
When going to much larger numbers of processes
this implementation may suffer from extra slow down due to
global synchronisation, as it was implemented using a blocking
\mpisendrecv{}.
%
A simple remedy may be to use nonblocking communications to allow
processes to receive particles and perform partial updates while
allowing send lists to be sent in the background.
