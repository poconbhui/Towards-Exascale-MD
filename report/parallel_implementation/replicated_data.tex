\subsection{Replicated Data}
\label{sec:replicated_data_implementation}

%
%Q: What data does the Replicated Data implementation use?
The \replicateddata{} scheme, as described in
\SEC{sec:background:subsec:replicated_data},
allocates a list of particles the size of
the entire MD system of particles
on each process.
%
It uses this list to keep an up-to-date copy of the system of particles
on every process.

In this section, the implementation details and performance of
of the \individualoperation{} and \pairoperation{} methods
for the \replicateddata{} scheme using MPI will be analysed and discussed.


%
% Replicated individual_operation v0
%

\subsubsection{Implementation of the \individualoperation{} Method}
\label{sec:replicated_data_individual_operation_implementation}

The \individualoperation{} method as outlined in
\SEC{sec:the_individual_operation_method}
is implemented by having each process update
its entire local list of particles.
%
As such, it closely resembles the example serial implementation.
%
This unparallelised approach involves more computation than
having each process evaluate a subsection of the list and
share the result with the other processes,
but it avoids a global synchronization.


%
%Q:What is the speedup of the Replicated Data individual_operation method?

%
% Overall speedup plot
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.individual_operation.logspeedup.plt%
    }
    \caption{
        \vZeroSpeedupCaption
            {\replicateddata{}}
            {\individualoperation{}}
    }
    \label{fig:v0_replicated_data_individual_operation_speedups}
\end{figure}


\vZeroSpeedupExplanation
    {\FIG{fig:v0_replicated_data_individual_operation_speedups}}
    {\replicateddata{}}
    {\individualoperation{}}

As can be seen from 
\FIG{fig:v0_replicated_data_individual_operation_speedups},
this method gains no speedup when extra cores are used.
%
It does, however, have an interesting jump in time around 8 cores.


%
%Q:What is the expected calculation time of the Replicated Data individual_operation method?
As each process performs an $\bigO{1}$ operation on $N$ particles with
the calculations are expected to take a time
\begin{equation}
\label{eqn:v0_replicated_individual_operation_overall_time}
    N\bigO{1} = \bigO{N}
\end  {equation}
%
%Q:What is the expected communication time of the Replicated Data individual_operation method?
As there are no MPI operations performed, the communications are
expected to take $\bigO{1}$ time.
%
The nonzero value is to allow for function call overheads and
branch evaluations as a result of evaluating benchmarking flags
within the code.
%
%Q:What is the expected overall time of the Replicated Data individual_operation method?
As a result, the overall time is ecpexted to take a time
\begin{equation}
    \bigO{N} + \bigO{1} = \bigO{N}
\end{equation}


%
% Individual breakdowns
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.individual_operation.512.time.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicateddata{}}
            {\individualoperation{}}
            {512}
    }
    \label{fig:v0_replicated_individual_operation_512_time}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.individual_operation.4096.time.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicateddata{}}
            {\individualoperation{}}
            {4096}
    }
    \label{fig:v0_replicated_individual_operation_4096_time}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.individual_operation.32768.time.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicateddata{}}
            {\individualoperation{}}
            {32768}
    }
    \label{fig:v0_replicated_individual_operation_32768_time}
\end  {figure}


\vZeroTimeExplanation
    {\FIG{fig:v0_replicated_individual_operation_512_time}}
    {\FIG{fig:v0_replicated_individual_operation_4096_time}}
    {\FIG{fig:v0_replicated_individual_operation_32768_time}}
    {\individualoperation{}}
    {\replicateddata{}}


%
%Q: Where and why does scaling stop for the Replicated Data individual_operation method?
From these, it is clear that
the time for the \individualoperation{} doesn't scale with the number
of cores and that MPI takes up no time as this method uses no MPI calls.

There is an interesting increase of time that occurs at 2 cores and again
at 8 cores.
%
Given that \hector{} has 4 \numa{} regions of 8 cores and each of those
regions is further subdivided into 4 \numa{} regions of 2 cores,
it is likely this is a cause for the jumps at 2 and 8 cores.
%
This is particularly noticeable in
\FIG{fig:v0_replicated_individual_operation_32768_time}
where the jump occurs at exactly the same core count, but is noticeably larger.

The jumps occur at the same numbers of processes in
\FIG{fig:v0_replicated_individual_operation_512_time},
\FIG{fig:v0_replicated_individual_operation_4096_time} and
\FIG{fig:v0_replicated_individual_operation_32768_time}
and after 8 processes, the time remains roughly constant.
%
This suggests either a latency effect with processes accessing memory
in other \numa{} regions or a memory bandwith effect.
%
Given that the effect scales roughly with the number of particles
in the system, at fixed core counts, it is most likely due to
memory bandwidth saturation.
%
If bandwidth saturated, it is expected that the time taken for
data transfer would
scale linearly with the size of the data per core being transferred.
%
Therefore, it is likely that the bandwidth is not saturated in
the 2 core \numa{} region
when only 1 core is in use, and
similarly that the bandwidth for
the 8 core \numa{} region is not saturated
when only 7 cores are in use.

Above 8 cores, the overall execution time for
the \individualoperation{} method for the \replicateddata{} implementation
appears to scale as $\bigO{N}$, independently of the number of cores used,
as predicted in
\EQN{eqn:v0_replicated_individual_operation_overall_time}.


%
% Replicated pair_operation v0
%

\subsubsection{Implementation of the \pairoperation{} Method}

The \pairoperation{} method as outlined in
\SEC{sec:the_pair_operation_method}
is implemented by having each process evaluating the pair
comparisons for a subset of the particles and sharing the
results with the other processes.

%
%Q: What is the speedup of the Replicated Data pair_operation method?

%
% Overall speedup plot
%
\begin{figure}[!h]
    \input{parallel_implementation/v0/replicated.pair_operation.logspeedup.plt}
    \caption{
        \vZeroSpeedupCaption
            {\replicateddata{}}
            {\pairoperation{}}
    }
    \label{fig:v0_replicated_data_pair_operation_speedups}
\end{figure}


\vZeroSpeedupExplanation
    {\FIG{fig:v0_replicated_data_pair_operation_speedups}}
    {\replicateddata{}}
    {\pairoperation{}}

The speedup shown in 
\FIG{fig:v0_replicated_data_pair_operation_speedups}
appears to continue up until around $P = N/2$ where it levels
off.
%
It is interesting to note that the number of cores that can be
used effectively appears to grow in proportion to the number of particles
in the system.

%
%Q:What is the expected calculation time of the Replicated Data pair_operation method?
Each process is designated a subset of the list of particles for which
it will evaluate the results of the comparison and reduction.
%
This looks similar to parallelising the outer loop of the example
serial implementation.
%
The number of particles each process is assigned is roughly $N/P$.
%
Each of these $N/P$ particles are compared to $N$ other particles
using an $\bigO{1}$ operation.
%
The time for one of the $N/P$ particles to be updated is
\begin{equation}
    N\bigO{1} = \bigO{N}
\end  {equation}
and so, the time for all $N/P$ particles to be updated is
\begin{equation}
    \frac{N}{P}\bigO{N} = \bigO{\frac{N^2}{P}}
\end  {equation}
As such, a calculation term of $\bigO{N^2/P}$ is expected.

%
%Q:What is the expected communication time of the Replicated Data pair_operation method?
After the each process finishes updating it's section of the list,
the updated sections of lists are shared across processes using
an \mpiallgatherv{}.
Assuming this uses a tree-like algorithm, this should perform
$\log{P}$ communications of $N$ particles with a constant latancy $l$
resulting in a communication time of
\begin{equation}
    \begin{split}
        \log{(P)}\,\bigO{N + l}
            &= \bigO{(N+l)\log{P}} \\
            &\approx{} \bigO{N\log{P}}
    \end{split}
\end{equation}

%
%Q:What is the expected overall time of the Replicated Data pair_operation method?
The overall execution time of this method should therefore be
\begin{equation}
    \bigO{\frac{N^2}{P}} + \bigO{N\log{P}}
        \approx{} \bigO{\frac{N^2}{P} + N\log{P}}
        \label{eqn:v0_replicated_pair_operation_overall_time}
\end{equation}


%
% Individual breakdowns
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.pair_operation.512.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicateddata{}}
            {\pairoperation{}}
            {512}
    }
    \label{fig:v0_replicated_pair_operation_512_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.pair_operation.4096.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicateddata{}}
            {\pairoperation{}}
            {4096}
    }
    \label{fig:v0_replicated_pair_operation_4096_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v0/%
        replicated.pair_operation.32768.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicateddata{}}
            {\pairoperation{}}
            {32768}
    }
    \label{fig:v0_replicated_pair_operation_32768_logtime}
\end  {figure}

\vZeroTimeExplanation
    {\FIG{fig:v0_replicated_pair_operation_512_logtime}}
    {\FIG{fig:v0_replicated_pair_operation_4096_logtime}}
    {\FIG{fig:v0_replicated_pair_operation_32768_logtime}}
    {\pairoperation{}}
    {\replicateddata{}}

%
%Q:Where and why does scaling stop for the Replicated Data pair_operation method?
In \FIG{fig:v0_replicated_pair_operation_512_logtime},
\FIG{fig:v0_replicated_pair_operation_4096_logtime} and
\FIG{fig:v0_replicated_pair_operation_32768_logtime},
it is apparent that scaling begins to drop off as the number
of processes used is comparable to the number of particles in the system.
%
From \EQN{eqn:v0_replicated_pair_operation_overall_time} when $P \ll{} N$
\begin{equation}
    \bigO{\frac{N^2}{P} + N\log{P}} \sim{} \bigO{\frac{N^2}{P}}
\end  {equation}
suggesting good scaling in this regime.
%
Similarly when $P \sim{} N$
or $P > N$
\begin{equation}
    \begin{split}
        \bigO{\frac{N^2}{P} + N\log{P}}
            &\sim{} \bigO{N + N\log{P}} \\
            &\sim{} \bigO{N\log{P}}
    \end{split}
\end{equation}
%
suggesting the communication term eventually dominating and
the overall time increasing as $N\log{P}$.

Indeed, examining where
\FIG{fig:v0_replicated_pair_operation_512_logtime},
\FIG{fig:v0_replicated_pair_operation_4096_logtime},
\FIG{fig:v0_replicated_pair_operation_32768_logtime} and
begin straying away from linear scaling,
it can be seen that the minimum execution time
of 512 particles is approximately $10^{-4}$ seconds,
of 4096 particles is approximately $10^{-3}$ seconds and
of 32768 particles is approximately $10^{-2}$ seconds.
%
This suggests that a minimum time solution may well scale as
$N\log{N}$ for the system sizes tested in the regime where $P \sim{} N$.
%
However, more data would need to be gathered to verify this claim.
