\section{Replicated Systolic Loop}


\subsection{The \individualoperation{} Method}

The \individualoperation{} method is impleemnted by having each process
update its local list of particles.

%
% Overall speedup plot
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.individual_operation.logspeedup.plt%
    }
    \caption{
        \vZeroSpeedupCaption
            {\replicatedsystolicloop{}}
            {\individualoperation{}}
            {$f(x) = \sqrt{x}$}
    }
    \label{fig:v1_replicated_systolic_loop_individual_operation_speedups}
\end{figure}


%
%Q: What is the speedup of the Replicated Systolic Loop individual_operation method?

\vZeroSpeedupExplanation
    {\FIG{fig:v1_replicated_systolic_loop_individual_operation_speedups}}
    {\replicatedsystolicloop{}}
    {\individualoperation{}}
    {$f(x) = \sqrt{x}$}

%
%Q: What is the expected calculation time of the Replicated Systolic Loop individual_operation method?

Given $S$ systolic elements per loop, each element is expected to
hold $N/S$ particles.
%
Performing an $\bigO{1}$ operation on each particle should take a time
\begin{equation}
    \frac{N}{S}\bigO{1} = \bigO{\frac{N}{S}}
\end{equation}
%
%Q: What is the expected communication time of the Replicated Systolic Loop individual_operation method?
As there are no MPI communications performed, the communications are expected
to take $\bigO{1}$ time.
%
As previously stated, this nonzero time is to allow for function
and branch evaluations as a result of benchmarking flags in the method.
%
%Q: What is the expected overall time of the Replicated Systolic Loop individual_operation method?
Combining the calculation and communication terms, this method
is expected to take a time
\begin{equation}
    \bigO{\frac{N}{S}} + \bigO{1} \approx{} \bigO{\frac{N}{S}}
\end{equation}

%
% Individual breakdowns
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.individual_operation.512.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicatedsystolicloop{}}
            {\individualoperation{}}
            {512}
    }
    \label{fig:v1_replicated_systolic_individual_operation_512_time}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.individual_operation.4096.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicatedsystolicloop{}}
            {\individualoperation{}}
            {4096}
    }
    \label{fig:v1_replicated_systolic_individual_operation_4096_time}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.individual_operation.32768.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption
            {\replicatedsystolicloop{}}
            {\individualoperation{}}
            {32768}
    }
    \label{fig:v1_replicated_systolic_individual_operation_32768_time}
\end  {figure}


\vZeroTimeExplanation
    {\FIG{fig:v1_replicated_systolic_individual_operation_512_time}}
    {\FIG{fig:v1_replicated_systolic_individual_operation_4096_time}}
    {\FIG{fig:v1_replicated_systolic_individual_operation_32768_time}}
    {\individualoperation{}}
    {\replicatedsystolicloop{}}

%
%Q: Where and why does scaling stop for the Replicated Systolic Loop individual_operation method?
It can be seen from
\FIG{fig:v1_replicated_systolic_individual_operation_512_time},
\FIG{fig:v1_replicated_systolic_individual_operation_4096_time} and
\FIG{fig:v1_replicated_systolic_individual_operation_32768_time}
that the time taken for this routine does indeed scale as $N$.
%
Along with 
\FIG{fig:v1_replicated_systolic_loop_individual_operation_speedups},
they also show polynomial scaling, consistent with a $\sqrt{P}$ scaling.

The implementation uses the \mpidimscreate{} function to generate a
cartesian topology.
%
This function attempts to create a 2d decomposition to $S \times{} R$
processes from the supplied $P$ processes where $S$ and $R$ are roughly
equal in size.
%
For $S$ and $R$ to be roughly equal, they should both be around $\sqrt{P}$.
%
That they can't always be decomposed into equal terms of $\sqrt{P}$
explains why the timings look stepped.
%
That the function constantly strives for equal terms of $\sqrt{P}$
explains why the general scaling trend follows $\sqrt{P}$.

As in the \systolicloop{} case, the MPI scaling terms are nonzero.
%
This is could, again, becaused by function call overhead and
branch evaluations caused by how benchmarking flags have been implemented.
%
Regardless,
the MPI scaling terms here are approximately constant and negligible.


\subsection{The \pairoperation{} Method}

The \pairoperation{} partitions systolic pulses to be performed among
the replica systolic loops, and has equivalent elements across loops
reduce their partial answers to generate an overall result for that
systolic element for that time step.

%
% Overall speedup plot
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.pair_operation.logspeedup.plt%
    }
    \caption{
        \vZeroSpeedupCaption
            {\replicatedsystolicloop{}}
            {\pairoperation{}}
            {$f(x) = x$}
    }
    \label{fig:v1_replicated_systolic_pair_operation_speedups}
\end{figure}


\vZeroSpeedupExplanation
    {\FIG{fig:v1_replicated_systolic_pair_operation_speedups}}
    {\replicatedsystolicloop{}}
    {\pairoperation{}}
    {$f(x) = x$}


%
%Q: What is the speedup of the Replicated Systolic Loop pair_operation method?
With the exception of the system size $N = 512$ particles,
\FIG{fig:v1_replicated_systolic_pair_operation_speedups}
displays near perfect speedup up to $P = N$.
%
The case of $N = 512$ displays rather good, if noisier speedup up
until $P = N$.
%
The most remarkable aspect of this data is that the $N = 512$ system can
be run up to $4096$ cores while still producing some
improvement in performance,
and actually run on $32768$ cores, albeit with a significant drop
in performance.
%
It appears that the speedup is improved for each system size over all of
the other parallelisation schemes previously presented
and can also be run well on an order of magnitude more cores than
there are particles.

%
%Q: What is the expected calculation time of the Replicated Systolic Loop pair_operation method?
As each replica systolic loop has $S$ systolic elements, each element
should have $N/S$ particles in its local array.
%
There should be similarly be $N/S$ particles in the receive array when
a partial reduction is being performed.
%
As with the systolic case, during one pulse, a partial reduction for one
particle using an $\bigO{1}$ comparison operation to compare it to the
$N/S$ particles in the receive array should take a time
\begin{equation}
    \frac{N}{S}\,\bigO{1} = \bigO{\frac{N}{S}}
\end{equation}
%
Performing this for the $N/S$ particles in the local list should take a time
\begin{equation}
    \frac{N}{S}\,\bigO{\frac{N}{S}} = \bigO{\left( \frac{N}{S} \right)^2}
\end{equation}
%
As each ring performs $S/R$ pulses per time step, the time for calculations
per time step should be
\begin{equation}
    \label{eqn:replicated_systolic_pair_operation_calculation_time}
    \begin{split}
        \frac{S}{R}\,\bigO{\left( \frac{N}{S} \right)^2}
            &= \bigO{\frac{S}{R} \frac{N^2}{S^2}} \\
            &= \bigO{\frac{N^2}{SR}} \\
            &= \bigO{\frac{N^2}{P}}
    \end{split}
\end{equation}
which is the same result as the \systolicloop{} case.


%
%Q: What is the expected communication time of the Replicated Systolic Loop pair_operation method?
As each process should perform $S/R$ pulses per time step, it performs
$S/R$ communications of $N/S$ particles with a constant latency $l$
introducing a time
\begin{equation}
    \label{eqn:replicated_systolic_pair_operation_pulse_time}
    \begin{split}
        \frac{S}{R}\,\bigO{\frac{N}{S} + l}
            &= \bigO{\left(\frac{N}{S} + l\right)\frac{S}{R}} \\
            &= \bigO{\frac{N}{R} + \frac{S}{R}}
    \end{split}
\end{equation}
%
After the pulses have been performed, each set of equivalent systolic
elements reduces their partial results using an \mpiallreduce{}.
%
Assuming a tree-like algorithm, this should perform $\log{R}$ communications
of $N/S$ particles with a constant latency $l$ introducing a time
\begin{equation}
    \label{eqn:replicated_systolic_pair_operation_reduce_time}
    \begin{split}
        \log{(R)}\,\bigO{\frac{N}{S} + l}
            &= \bigO{\log{(R)}\, \left(\frac{N}{S} + l\right)} \\
            &= \bigO{\frac{N}{S}\log{R} + \log{R}}
    \end{split}
\end{equation}
%
Combining
\EQN{eqn:replicated_systolic_pair_operation_pulse_time} and
\EQN{eqn:replicated_systolic_pair_operation_reduce_time},
the overall communication time is
\begin{equation}
    \label{eqn:replicated_systolic_pair_operation_communication_time}
    \bigO{\frac{N}{R} + \frac{S}{R}} + \bigO{\frac{N}{S}\log{R}}
        =
        \bigO{\frac{N}{R} + \frac{S}{R} + \frac{N}{S}\log{R} + \log{R}}
\end{equation}

%
%Q: What is the expected overall time of the Replicated Systolic Loop pair_operation method?
Combing the results of 
\EQN{eqn:replicated_systolic_pair_operation_calculation_time} and
\EQN{eqn:replicated_systolic_pair_operation_communication_time},
the overall time for this method to complete a time step should be
\begin{equation}
    \label{eqn:replicated_systolic_pair_operation_overall_time}
    \begin{split}
    \bigO{\frac{N^2}{SR}}
        + \bigO{\frac{N}{R} + \frac{S}{R} + \frac{N}{S}\log{R} + \log{R}} \\
        = \bigO{\frac{N^2}{SR}
        + \frac{N}{R} + \frac{S}{R} + \frac{N}{S}\log{R} + \log{R}}
    \end{split}
\end{equation}
%
As an aside, when $S = P$ and $R = 1$, this overall time reverts
to the overall time predicted by the \systolicloop{} implementation.
%
It can be seen that the $S/R$ term is equivalent to the communication
term proportional to $P$ in the \systolicloop{} case.
%
Replication here tackles this communication term, previously identified as
the bottleneck of the \systolicloop{} scheme, replacing the term
proportional to $P$ with one proportional to $S/R$ and another
proportional to $(N/S)\log{R}$.


%
% Individual breakdowns
%
\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.pair_operation.512.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption{\replicatedsystolicloop{}}{\pairoperation{}}{512}
    }
    \label{fig:v1_replicated_systolic_pair_operation_512_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.pair_operation.4096.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption{\replicatedsystolicloop{}}{\pairoperation{}}{4096}
    }
    \label{fig:v1_replicated_systolic_pair_operation_4096_logtime}
\end  {figure}

\begin{figure}[!h]
    \input{%
        parallel_implementation/v1/%
        replicated_systolic.pair_operation.32768.logtime.plt%
    }
    \caption{
        \vZeroTimeCaption{\replicatedsystolicloop{}}{\pairoperation{}}{32768}
    }
    \label{fig:v1_replicated_systolic_pair_operation_32768_logtime}
\end  {figure}

\vZeroTimeExplanation
    {\FIG{fig:v1_replicated_systolic_pair_operation_512_logtime}}
    {\FIG{fig:v1_replicated_systolic_pair_operation_4096_logtime}}
    {\FIG{fig:v1_replicated_systolic_pair_operation_32768_logtime}}
    {\individualoperation{}}
    {\replicatedsystolicloop{}}


%
%Q: Where and why does scaling stop for the Replicated Systolic Loop pair_operation method?
\FIG{fig:v1_replicated_systolic_pair_operation_512_logtime} shows that
communications begin to approach calculation times around $N/P = 2$
and dominate around $N/P = 0.5$.
%
\FIG{fig:v1_replicated_systolic_pair_operation_4096_logtime} shows
communications approaching calculation times around $N/P = 1$ and
dominating around $N/P = 0.25$.
%
\FIG{fig:v1_replicated_systolic_pair_operation_32768_logtime} shows
communications don't appear to intersect with calculations at all,
although some small deviation of the total run time from the calculation
time can be seen starting around $N/P = 2$.
%
This may be due to synchronisation issues at such a large core count.

The implementation uses the \mpidimscreate{} function to generate
the $S\times{}R$ decomposition of $P$.
%
As this function strives to
produce an $S$ and $R$ of roughly equal size, and each tick on
\FIG{fig:v1_replicated_systolic_pair_operation_512_logtime},
\FIG{fig:v1_replicated_systolic_pair_operation_4096_logtime} and
\FIG{fig:v1_replicated_systolic_pair_operation_32768_logtime}
is a perfect square, the simplification $S = R = \sqrt{P}$
can be made to describe the general trend of the graphs.
%
\EQN{eqn:replicated_systolic_pair_operation_overall_time}
becomes
\begin{equation}
    \label{eqn:replicated_systolic_pair_operation_simplified_time}
    \begin{split}
        \bigO{
            \frac{N^2}{P}
            + \frac{N}{\sqrt{P}}
            + \frac{\sqrt{P}}{\sqrt{P}}
            + \frac{N}{\sqrt{P}}\log{\sqrt{P}}
            + \log{\sqrt{P}}
        } \\
        = \bigO{
            \frac{N^2}{P}
            + \frac{N}{\sqrt{P}}(\log{\sqrt{P}}+1)
            + \log{\sqrt{P}}
        }
    \end{split}
\end{equation}

The calculation term of $N^2/P$ term suggests
the initial calculation time scales as $N^2$
and the communication term $(N/\sqrt{P}) (\log{\sqrt{P}}+1)$ suggests
the initial communication time scales as $N$.
%
\FIG{fig:v1_replicated_systolic_pair_operation_512_logtime},
\FIG{fig:v1_replicated_systolic_pair_operation_4096_logtime} and
\FIG{fig:v1_replicated_systolic_pair_operation_32768_logtime}
appear to support this.


\begin{figure}
    \input{parallel_implementation/replicated_systolic_comm_fits.plt}
    \caption{
        Communication times for systems of particles of size
        $N=512$, $N=4096$ and $N=32768$ fitted to the function
        $a\times{}N(\log{\sqrt{P}} + 1)/\sqrt{P} + b\times{}\log{\sqrt{P}}$
        corresponding to the predicted communication time.
    }
    \label{fig:replicated_systolic_communication_fits}
\end{figure}

The communication times for the three system sizes along with a
system of size $N = 262144$ are plotted in
\FIG{fig:replicated_systolic_communication_fits}
along with fittings to the suggested trends.
%
An interesting note is that the suggested communications complexity
appears to fit well to the graph, however, appears to be
missing some parameters.

In particular, the cofactor of the latency term appears to grow
with the number of particles communicated.
%
The communication time at $P = 32768$ cores
appears to increase by only one order of magnitude across an
increase of two orders of magnitude in the number of particles in the system
from $N = 512$ to $N = 32768$.
%
However, the change does appear to increase by an order of magnitude between
$N = 32768$ and $N = 262144$.
%
As the latency term should be independent of the number of particles,
this should remain a constant value.
%
This plot would suggest the communications scaling looks closer to
$\sqrt{N}\log{\sqrt{P}}$.
%
There is, however, not enough data present to draw any
conclusions here.


