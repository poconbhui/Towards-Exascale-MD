\ifx\macrosHeader\undefined

%
% Styles
%
\newcommand{\subsubsubsection}[1]{{\bf #1} \vspace{0.5\baselineskip}}


%
% Copywriting
%
\newcommand{\padraigoconbhui}{P\'{a}draig \'{O} Conbhu\'{\i}}
\newcommand{\plainpadraigoconbhui}{Padraig O Conbhui}
\newcommand{\towardsexascalemd}{Towards Exascale Molecular Dynamics}

\newcommand{\bigO}[1]{\mathcal{O}{\left({ #1 }\right)}}

\newcommand{\hector}{HECToR}
\newcommand{\velocityverlet}{velocity Verlet}
\newcommand{\verletlist}{Verlet list}
\newcommand{\twobody}{two body}
\newcommand{\LennardJones}{Lennard-Jones}
\newcommand{\numa}{NUMA}

\newcommand{\openmp}{OpenMP}

\newcommand{\abstractdistribution}{abstract distribution}
\newcommand{\replicateddata}{replicated data}
\newcommand{\systolicloop}{systolic loop}
\newcommand{\sharedandreplicateddata}{shared and replicated data}
\newcommand{\replicatedsystolicloop}{replicated systolic loop}

\newcommand{\mpidimscreate}{%
    MPI\_\discretionary{-}{}{}%
    Dims\_\discretionary{-}{}{}%
    create%
}
\newcommand{\mpiallreduce}{%
    MPI\_\discretionary{-}{}{}%
    All\discretionary{-}{}{}%
    reduce%
}
\newcommand{\mpiallgatherv}{%
    MPI\_\discretionary{-}{}{}%
    All\_\discretionary{-}{}{}%
    gatherv%
}
\newcommand{\mpisendrecv}{%
    MPI\_\discretionary{-}{}{}%
    Send\discretionary{-}{}{}%
    recv%
}
\newcommand{\mpicommsplit}{%
    MPI\_\discretionary{-}{}{}%
    Comm\_\discretionary{-}{}{}%
    split%
}

\newcommand{\individualoperation}{%
    \texttt{indi}\discretionary{-}{}{}%
    \texttt{vidual\_}\discretionary{-}{}{}%
    \texttt{oper}\discretionary{-}{}{}%
    \texttt{ation}%
}
\newcommand{\pairoperation}{%
    \texttt{pair\_}\discretionary{-}{}{}%
    \texttt{oper}\discretionary{-}{}{}%
    \texttt{ation}%
}

\newcommand{\EQN}[1]{Eqn.~(\ref{#1})}
\newcommand{\LST}[1]{Listing~\ref{#1}}
\newcommand{\FIG}[1]{Figure~\ref{#1}}
\newcommand{\SEC}[1]{Section~\ref{#1}}


\newcommand{\vZeroSpeedupCaption}[2]{
    {\bf Speedup vs. Cores} for the {\bf #1} implementation of the
    {\bf #2} method for systems of particles of size
    $N=512$ (red), $N=4096$ (green) and $N=32768$ (blue) and the
    function $f(x) = x$ (light red).
}

\newcommand{\vZeroSpeedupExplanation}[3]{
    #1 shows how the speedup of the #2 implementation of the
    #3 method scales with the number of cores available to the
    implementation.
    %
    This is presented for systems of particles of size
    $N=512$ shown in red, $N=4096$ shown in green
    and $N=32769$ shown in blue along with the function
    $f(x) = x$ shown in light red.
}

\newcommand{\vZeroTimeCaption}[3]{
    {\bf Time vs. Cores} for the {\bf #1} implementation of the
    {\bf #2} method for the total execution time (red),
    calculation execution time without MPI (green) and
    MPI execution time without calculations (blue)
    for an MD system of particles of size $\bf N = {#3}$.
}

\newcommand{\vOneSRTimeCaption}[3]{
    \vZeroTimeCaption{#1}{#2}{#3}
    %
    %The number cores presented is the number of MPI
    %processes used multiplied by the number of \openmp{} threads
    %available per MPI process.
}

\newcommand{\vZeroTimeExplanation}[5]{
    {#1}, {#2} and {#3}
    show how the execution time of the {#4} method
    for the {#5} scheme varies with
    the number of cores used for the simulation
    where $N$, the number of particles in the system, is
    512, 4096 and 32768 respectively.
    They present the cases where
    both MPI and calculations are performed (red)
        representing the total execution time;
    calculations are performed without MPI (green)
        representing the execution time of the simulation
        without communication effects; and
    MPI is performed without calculations (blue)
        representing the time taken up solely by the communications.
}

\newcommand{\vOneSRTimeExplanation}[5]{
    \vZeroTimeExplanation{#1}{#2}{#3}{#4}{#5}
    %
    In this graph, the cores listed represent the total possible cores
    $P = P_{MPI} \times{} P_{OMP}$ available to the implementation.
}


\def\macrosHeader{0}
\fi
