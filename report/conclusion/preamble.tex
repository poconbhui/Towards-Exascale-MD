%
%Q: What were the objectives and findings of the dissertation?
This dissertation aimed to evaluate the scaling bottlenecks of
some parallel schemes used to perform \twobody{} MD simulations.
%
Two base schemes were chosen:
the \replicateddata{} and \systolicloop{} schemes.
Two improvements upon these schemes were also proposed:
the \sharedandreplicateddata{} and \replicatedsystolicloop{} schemes.

The steps taken to determine the bottlenecks were:
\begin{itemize}
\item
    Proposing an implementation pattern capable of representing
    all four of the parallel schemes to ensure all implementations
    were comparable.

\item
    Implementing the four parallel schemes using the prescribed
    implementation pattern.

\item
    Determining the expected time complexity of the
    \individualoperation{} and \pairoperation{} proposed by
    the pattern, taking
    account of expected calculation and communication patterns.

\item
    Writing a benchmark for the
    \individualoperation{} and \pairoperation{} methods
    to run each for a minimum of 5 iterations and minimum of 1 second
    in 3 regimes:
    \begin{itemize}
        \item Normal operation
        \item MD calculations only
        \item MPI calls only
    \end{itemize}

\item
    Running this benchmark on \hector{} Phase 3, using 32 cores per node
    where possible, on a range from 1 to 32768 cores.

\item
    Matching the graphs found to the expected time complexities.
    Determining where communications become a significant overhead
    to the calculation, and whether this was an effect of communication
    times or of latency.
\end{itemize}


This dissertation presents a very simple improvement that can be
made to an MPI implementation of the \replicateddata{} scheme to 
immediately reduce the overheads of data storage and MPI
communications.
%
Considering only one unoptimised \openmp{} directive was added to
one loop, it is a very minimal change to make.

The dissertation proposes the \replicatedsystolicloop{}.
%
Given the $\log{\sqrt{P}}$ scaling of this implementation, and the
improvement on memory requirements per node over the \replicateddata{}
schemes, this method improves upon both the \replicateddata{} and
\systolicloop{} schemes.
%
It also passes dramatically smaller messages across the machine
in a much more limited fashion.
%
Systolic pulses are limited to neighbouring processes, and the final
reduction is performed across distinct sets of communicators.
%
It may be possible to take advantage of this topology to gain
improvements in communication times.

Where a \systolicloop{} implementation exists, much of the code can
be reused, requiring only minor changes.
%
The \replicatedsystolicloop{} adds only two extra
communicators, one of which is used as in the \systolicloop{} case
for performing systolic pulses.
%
It then determines pulse chunks to be performed, and performs
an initial swap.
%
After that, the usual \systolicloop{} code can be used, albeit performing
fewer pulses.
%
The \replicatedsystolicloop{} then includes an \mpiallreduce{} to determine
the final value.



\section{Findings}

It was found for the \replicateddata{} and the \sharedandreplicateddata{}
implementations that
data transfer between MPI processes was the greatest bottleneck.
%
It was found for the \systolicloop{} and the \replicatedsystolicloop{}
implementations that
communication latency between MPI processes was the greatest bottleneck.
%
The bottleneck presented by the \systolicloop{} scheme was in
the number of pulses performed per time step.
%
For the \replicatedsystolicloop{}, this was in the reduction across
equivalent systolic elements.

The proposed \sharedandreplicateddata{} scheme was found to perform at
least as well as the \replicateddata{} scheme.
%
It was  shown to require
an amount of memory inversely proportional to the number of
\openmp{} threads per MPI process used.
%
It also displayed a small improvement in performance due to
reduced MPI communications for similar total core counts.

The proposed \replicatedsystolicloop{} scheme was found to directly
improve upon both the \replicateddata{} and \systolicloop{} schemes.
%
It was shown that the memory requirement scaling was similar to
the \systolicloop{} scheme, with memory per core as $N/\sqrt{P}$,
meaning it was suitable for running data sets that are to large to
fit on any one node.
%
It also displayed a marked improvement in communication times,
approaching $\log{\sqrt{P}}$ at large $P$.



\section{Future Work}

The \replicatedsystolicloop{} presented here may be of particular
interest for future work.
%
In this dissertation, the placement of processes to take advantage
of communication patterns presented by this scheme has been neglected.
%
Considerations of the nearest neighbour placement of nodes during the
systolic pulse may be of interest.
%
So, too, might optimising for the reduction operation.

An interesting area of study may be optimising the balance of
systolic elements to replica loops.
%
In this dissertation \mpidimscreate{} was used to determine the
number of systolic elements per loop and the number of loops.
%
This function strove to keep these two numbers equal.
%
This resulted in the number of systolic pulses performed remaining
near constant at around 1.
%
However, this lead to the eventually dominating $\log{\sqrt{P}}$
communication term, which was in fact $\log{R}$.

By reducing the number of replicas and increasing the number of
systolic elements, the number of systolic pulses will increase.
%
This will lead to an increase in the $S/R$ communication term.
%
However, before the $S/R$ term dominates, the $\log{R}$ term will
decrease.
%
The decrease in this dominant term may result in an improved
communication time.

The implementation presented here is used to evaluate direct
solutions to the equations of motion.
%
It would be interesting to see how it fares when it is used in
conjunction with a truncation scheme.
%
Using the $S/R$ ratio considered here, each process could
keep a \verletlist{} for $2N/\sqrt{P}$ particles, as only one
or two systolic pulses should be occurring.
%
The reduction in calculation time as a result of this may
provide some more interesting results for the interplay
of communications and computations.
%
It is expected, however, for this scheme to perform better than
an equivalent \replicateddata{} and \verletlist{} scheme as
the communication bottleneck here does not appear to grow linearly
with $N$ and the number of particles to be stored in the \verletlist{}
should reduce with the number of processes used.


\section{Evaluation}

This dissertation presented some scaling results for some implementations
and variations of the \replicateddata{} and \systolicloop{} schemes.
%
The lack of comparisons to any real-world code makes it difficult
to place the overall performance of these codes.
%
This presents a difficulty when attempting to place the timings
made in terms of realistic values.
%
A further problem with placing these timings is that timings presented
for real-world code are often for schemes including truncation.
%
As a result some information is lost in this dissertation due to
the computationally intensive calculation performed.

As the focus here was not on implementing particularly performant
MD simulations, but rather evaluating the bottlenecks of particular
parallel schemes, any comparison to the times of real-world code
would not be interesting.
%
As a result, the overall time of the code is less interesting than
the trends produced by the timings.
%
Here, these trends have been presented in a way that the
implementations may be directly compared to one another.

The communication times presented here take only data transfer and
latency times into account.
%
Indeed, in the strong scaling graphs for the \replicatedsystolicloop{}
scheme, it was seen that the total time began to deviate from the
calculation time for large $P$, despite the fact that the measured
communication times were almost an order of magnitude lower than
either the total or the calculation times.
%
It was suggested that this may be due to processes waiting at
synchronisation barriers, namely at the \mpiallreduce{} used
by this scheme.
%
However, it is difficult to make any solid conclusions on this based
on the data presented.

This dissertation presented the \replicatedsystolicloop{} which is,
to the best of the author's knowledge, a new approach to the problem.
%
It proposes a data distribution scheme that is suitable for working
with data that exceeds the memory of any one node.
%
It also proposes a communications scheme that improve upon that
of the \replicateddata{} scheme.
%
The data gathered suggests that the communication time at high core
counts scale as $\log{\sqrt{P}}$ and scales sublinearly with the
number of particles.
the system, and may be mostly latency based.
