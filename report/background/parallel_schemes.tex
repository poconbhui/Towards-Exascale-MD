\section{Parallel Schemes}

%
%Q: Can we parallelize the force calculation?
The \velocityverlet{} scheme with a \twobody{} potential
immediately provides an algorithm for
performing MD simulations of long ranged forces in isolated systems.
%
This dissertation will explore parallel implementations of this algorithm.
%
Here, the \replicateddata{} and \systolicloop{} schemes will be outlined.



\subsection{Replicated Data}
\label{sec:background:subsec:replicated_data}

%
%Q: What is the replicated data scheme?
In the \replicateddata{} scheme, illustrated in
\FIG{fig:replicated_data_illustration},
the entire system of particles is replicated across all processors.
%
This is useful for the force update, as each particle must have a view of
every other particle to determine its instantaneous force.
%
Given $P$ replicated processes, for each process
determines the force for $1/P$ of the particles.
%
The processes then share these newly determined forces with each other
such that each processor will have a copy of the system where every
particle has its force set to the correct value for that time step.
%
The processes may then step their system forward in time.
%
In this way, the \replicateddata{} scheme may provide
an $\bigO{N^2/P}$ algorithm.


\begin{figure}[!ht]
    \begin{center}
    \begin{tikzpicture}[scale=.5]
        \newcommand{\drawRect}[1]{
            \draw [block] (6*#1,0) rectangle (6*#1+4,4);
            \draw [block, redfill] (6*#1,#1) rectangle (6*#1+4,#1+1);
        }
        % Draw replicated blocks and force areas
        \drawRect{0}
        %
        \drawRect{1}
        %
        \drawRect{2}
        %
        \drawRect{3}
        %
        % Draw arrows
        \newcommand{\drawArr}[1]{
            \draw [latex'-latex', thick] (4 + 6*#1+0.2,2) -- (4 + 6*#1+1.8,2);
        }
        \drawArr{0}
        \drawArr{1}
        \drawArr{2}
        %
        % Draw home block
%        \draw [block] (9,8) rectangle (13,12);
%        \newcommand{\drawFromHome}[1]{
%            \draw [->, thick] (9.5+#1,8-0.2) -- (2 + 6*#1,4+0.2);
%        }
%        \drawFromHome{0}
%        \drawFromHome{1}
%        \drawFromHome{2}
%        \drawFromHome{3}
        %
        % Draw text
%        \node at (11,13) {Initial system};
        %\node at (11, -2) {Replicas};
        \newcommand{\drawReplicaNum}[1]{
            \node at (2 + 6*#1, -1) {#1};
        }
        \drawReplicaNum{0}
        \drawReplicaNum{1}
        \drawReplicaNum{2}
        \drawReplicaNum{3}
    \end{tikzpicture}
    \end{center}
    %
    \caption{
        An outline of the replicated data scheme.
        Each process, numbered 0-4,
        holds a copy of the entire system of particles,
        represented here in blue.
        Each process is responsible for updating a subset of the
        particles in the system, represented here in red.
        Processes then communicate these changes to each other
        to keep each copy of the system up to date.
    }
    \label{fig:replicated_data_illustration}
\end{figure}


%
%Q: What are the scaling challenges of replicated data?
When communication times are taken into account, however, it becomes
clear that this scheme will not scale as $\bigO{N^2/P}$ indefinitely.
%
An immediate concern is that each process must have the same view of the
system at all times.
%
As a result, all processes must share their updated particles at
every time step and receive updated particles from other processes.
%
This results in a number of system-wide of communications occurring
at every time step,
with $\log{P}$ communications of $N$ particles
yielding a communication time of $\bigO{N \log{P}}$.

As every process has an individual copy of the system of particles,
when large systems are simulated using a large number of cores,
the memory of the machine it is running on can be stretched rather thin.
%
For a system of particles where each particle uses double
precision floating point (8~B) data to store
a position vector,
a velocity vector,
a force vector and
a scalar mass,
amounting to 80~B per particle, a 32~GB system could hold around $10^8$
particles.
%
However, if the system has around 10 cores per 32~GB of memory,
with the replicated scheme,
it can now hold a maximum of around $10^7$ particles
as each core will be holding a full system.
%
As a result, the maximum possible size of the system is
inversely proportional to
the number of cores used per GB of memory per node.

We further encounter the problem that all processes must communicate
with every other process at every time step.
%
This puts global synchronisation points into the simulation.
%
Frequent global synchronisation and global data exchange can incur
significant overhead for the calculation.

%
%Q: Why are the scaling challenges of replicated data concerning for exascale?
These drawbacks are particularly important when considering exascale systems
where systems are expected to have large numbers of cores per node.
%
As memory per node is expected to grow at a much lower rate than
the number of cores per node,,
a \replicateddata{} scheme may have difficulties
utilising every core on a node.
%
Indeed, even in current systems, nodes are often requested and only a
subset of cores used due to memory per core considerations in applications.
%
Even if all the cores in the node are utilised, frequent global synchronisation
points may not scale well to large numbers of processes.



\subsection{Systolic Loop}
\label{sec:background:subsec:systolic_loop}

%
%Q: What is the systolic loop scheme?
In the \systolicloop{} scheme, the system is split up and distributed among
all the processes.
%
Force updates are performed by passing list data around a ring in pulses.
%
This is illustrated in \FIG{fig:systolic_loop_illustration}.

\begin{figure}[!ht]
    \begin{center}
    \begin{tikzpicture}
        \newcommand{\smallRectangle}[3]{
            \draw [block, #3] (#1+0.1,#2+0.1) rectangle (#1+1-0.1,#2+1-0.1);
        }
        %
        % Draw bounding boxes
        \draw [draw, rounded corners] (-0.5,0) rectangle (1.5, 1);
        %
        \draw [draw, rounded corners] (2,1.5) rectangle (3, 3.5);
        \draw [draw, rounded corners] (4,1.5) rectangle (5, 3.5);
        %
        \draw [draw, rounded corners] (5.5,0) rectangle (7.5, 1);
        %
        \draw [draw, rounded corners] (4,-0.5) rectangle (5, -2.5);
        \draw [draw, rounded corners] (2,-0.5) rectangle (3, -2.5);
        %HEREHEREHEREHERE
        %
        % Draw outer ring
        \smallRectangle{-0.5}{0}{bluefill}
        %
        \smallRectangle{2}{2.5}{bluefill}
        \smallRectangle{4}{2.5}{bluefill}
        %
        \smallRectangle{6.5}{0}{bluefill}
        %
        \smallRectangle{4}{-2.5}{bluefill}
        \smallRectangle{2}{-2.5}{bluefill}
        %
        % Draw inner ring
        \smallRectangle{0.5}{0}{redfill}
        %
        \smallRectangle{2}{1.5}{redfill}
        \smallRectangle{4}{1.5}{redfill}
        %
        \smallRectangle{5.5}{0}{redfill}
        %
        \smallRectangle{4}{-1.5}{redfill}
        \smallRectangle{2}{-1.5}{redfill}
        %
        % Draw circle
        \newcommand{\off}{0.1}
        \path [line] (1, 0.5+\off) |- (2.5, 2);
        %
        \path [line] (2.5+\off, 2) -- (4.5, 2);
        \path [line] (4.5+\off, 2) -| (6, 0.5);
        %
        \path [line] (6, 0.5-\off) |- (4.5, -1);
        %
        \path [line] (4.5-\off, -1) -- (2.5, -1);
        \path [line] (2.5-\off, -1) -| (1, 0.5);
        %
        % Draw labels
        \newcommand{\drawNode}[3]{
            \node at (#1, #2) {#3};
        }
        \drawNode{-1}{0.5}{0}
        %
        \drawNode{2.5}{4}{1}
        \drawNode{4.5}{4}{2}
        %
        \drawNode{8}{0.5}{3}
        %
        \drawNode{4.5}{-3}{4}
        \drawNode{2.5}{-3}{5}
    \end{tikzpicture}
    \end{center}
    \caption{
        A representation of the systolic loop model.
        Each process, numbered 0-5, holds a subset of the entire
        system of particles, represented in blue.
        Processes pass packets of particles, represented in red,
        around the loop.
        When a process receives a packet, it partially updates its local
        system of particles.
        When it has received every packet, it will have fully updated
        its local particles.
    }
    \label{fig:systolic_loop_illustration}
\end{figure}

Initially, a process will make a copy of its list of particles,
and then use that to perform a partial force update on its local list.
%
It will then pass the copied list to the ``right'' process and
receive a new list from the ``left'' process and use that to perform
another partial force update on its local list.
%
These ``pulses'' continue until a process receives its own system again from
the ``left'' process, signifying that it has at some stage received
a copy of every list on every process.
%
At this point, it should have added the force contributions from
every particle in the system to the particles in its local list.
%
From here, a process may update the position and velocity of these local
particles, thereby stepping the system forward.
%
As comparing lists of particles on each pulse is an $\bigO{(N/P)^2}$ operation
and each time step requires $P$ pulses, the calculation is expected to take
$\bigO{N^2/P}$ time.


%
%Q: What are the scaling challenges of systolic loop?
As the system is distributed over a number of processes, this allows
us to work on very large systems.
%
Taking the systems and figures from the previous section,
rather than the simulation being limited to $10^7$ particles for the system,
it is instead limited to $10^7$ particles per core per 32~GB node.
%
As a result, there is an upper limit on particles per core, but no
upper limit on the actual system being studied.

Another benefit is that
communications between processes are more limited than
the replicated case.
%
Here, each process communicates with only two other neighbouring processes
and sends and receives only $N/P$ particles.
%
By this mechanism, volume of data
being communicated across the machine is limited
and and there are no hard global synchronisation points.
%
However, the number of communications performed per time step
scales linearly with the number of processes.

When factoring in the number of systolic pulses per time step,
it becomes clear that the $\bigO{N^2/P}$ scaling will not last indefinitely.
%
As a result of communication latency, a communication term of
$\bigO{P}$ exists.
%
As the calculation scales inversely with the number of processes
and the communication linearly, it is clear that communication time will
eventually dominate over calculation.
%
Even for very large $N$, a modest $P$ can incur large latency overheads.
%
%Q: Why are the scaling challenges of systolic loop concerning for exascale?
As increasing the number of processes to a large value
can quickly result in calculations having significant communication overhead,
this raises an immediate concern for exascale system.
