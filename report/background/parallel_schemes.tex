esubsection{Parallel Schemes}
\FIG{fig:v0_systolic_pair_operation_512_logtime},
\FIG{fig:v0_systolic_pair_operation_4096_logtime} and
\FIG{fig:v0_systolic_pair_operation_32768_logtime}

%
%Q: Can we parallelize the force calculation?
As discussed, the \velocityverlet{} scheme
immediately provides an algorithm for
performing MD simulations of long ranged forces in isolated systems.
%
We would like to explore parallel implementations of this algorithm.
%
In this dissertation, we begin by focusing on two particular schemes
that directly evaluate the \velocityverlet{} algorithm:
\replicateddata{} and the \systolicloop{}.



\subsubsection{Replicated Data}
\label{sec:background:subsec:replicated_data}

%
%Q: What is the replicated data scheme?
In the \replicateddata{} scheme,
the entire system of particles is replicated across all processors.
%
This is useful for the force update, as each particle must have a view of
every other particle to determine its instantaneous force.
%
The general approach is, in $P$ replicated processes, for each processor
to determine the force for $1/P$ of the particles.
%
Each process may then either update the particles whose forces it has
determined and then share them, or share them and then update the entire
list of particles.
%
In this way, it may provide an $\bigO{N^2/P}$ algorithm.

%
%Q: What are the scaling challenges of replicated data?
Ideally, we would like to simply use $P = N^2$ processes and produce an
$\bigO{1}$ algorithm, however, this is not possible for several reasons.
%
An immediate concern is that each process must have the same view of the
system at all times.
%
As a result, all processes must share their updated particles at
every time step and receive updated particles from other processes.
%
This results in a large amount of communications occurring very frequently,
with a size growing as $\bigO{N}$ and as $\bigO{\log{P}}$,
yielding a communication time of $\bigO{N \log{P}}$.

Further problems arising from this algorithm is that every process has
an individual copy of the system.
%
For large systems running on a large number of cores,
this can stretch the memory of the machine it is running on.
%
Imagining a system of particles where each particle contains double
precision floating point (8~B) data for position, velocity, force and mass,
amounting to 80~B per particle, a 32~GB system could hold around $10^8$
particles.
%
However, if the system has around 10 cores per 32~GB of memory,
with the replicated scheme,
it can now hold a maximum of around $10^7$ particles
as each core will be holding a full system.
%
Essentially, the maximum size of the system becomes inversely proportional
to the number of cores used per GB of memory.
%
This increased use of memory can cause other issues such as poor caching
due to the increased memory use.

We further encounter the problem that all processes must communicate
with every other process at every time step.
%
This puts global synchronisation points into the simulation.
%
Frequent global synchronisation and global data exchange can incur
significant overhead for the calculation.

%
%Q: Why are the scaling challenges of replicated data concerning for exascale?
These drawbacks are particularly important when considering exascale systems
where we expect large numbers of cores per node.
%
As we expect memory per node to grow at a much lower rate, a \replicateddata{}
scheme may have difficulties utilising every core on a node.
%
Indeed, even in current systems, nodes are often requested and only a
subset of cores used due to memory per core considerations in applications.
%
Even if all the cores in the node are utilised, global synchronisation
points do not scale well to large numbers of processes.



\subsubsection{Systolic Loop}
\label{sec:background:subsec:systolic_loop}

%
%Q: What is the systolic loop scheme?
In the \systolicloop{} scheme, the system is split up and distributed among
all the processes.
%
Force updates are performed by passing list data around a ring in pulses.

Initially, a process will make a copy of its list of particles,
and then use that to perform a partial force update on its local list.
%
It will then pass the copied list to the ``right'' process and
receive a new list from the ``left'' process and use that to perform
another partial force update on its local list.
%
This process continues until a process receives its own system again from
the ``left'' process, signifying that it has at some stage received
a copy of every list on every process.
%
At this point, it should have added the force contributions from
every particle in the system to the particles in its local list.
%
From here, a process may update the position and velocity of these local
particles, thereby stepping the system forward.
%
As we are performing an $\bigO{\left(\frac{N}{P}\right)^2}$ operation
when comparing particles and perform this $\bigO{P}$ times, we
get an $\bigO{N^2/P}$ algorithm.


%
%Q: What are the scaling challenges of systolic loop?
As the system is distributed over a number of processes, this allows
us to work on very large systems.
%
Taking the systems and figures from the previous section,
rather than being limited to $10^7$ particles for the system,
we are limited to $10^7$ particles per core.
%
As a result, we have an upper limit on particles per core, but no
upper limit on the actual system being studied.
%
Further, communications between processes is more limited than the
replicated case.
%
Here, we have each process communicating with only two other processes
and sending only $N/P$ particles.
%
By this mechanism, we simultaneously reduce the volume of data
being communicated between every process and
remove a hard global synchronisation
when compared to the \replicateddata{} scheme.
%
However, we must perform $P$ communications.

Ideally, we could set $P = N$ and have a $\bigO{1}$ operation performed
$\bigO{P = N}$ times.
%
The primary limitation here is the overhead of performing the systolic
pulses.
%
As communications have a given latency, we would like to reduce the number
of communications performed.
%
In a \systolicloop{}, the number of operations performed reduces with
the square of the processes, but the communications performed scales linearly.
%
As such, we quickly reach a point where latency dominates over computation.
%
Even for very large $N$, a modest $P$ can incur large latency overheads.
%
%Q: Why are the scaling challenges of systolic loop concerning for exascale?
As increasing the number of processes to a large value
can quickly result in calculations having significant latency,
we see an immediate concern for exascale system.
