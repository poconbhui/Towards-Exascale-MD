\section{Parallel Schemes}
\label{sec:background:subsec:parallel_schemes}

%
%Q: Can we parallelize the force calculation?
The \velocityverlet{} scheme with a \twobody{} potential
immediately provides an algorithm for
performing MD simulations of long ranged forces in isolated systems.
%
This dissertation will explore parallel implementations of this algorithm.
%
%Q: What force parallelization schemes exist?
For the purposes of this text,
parallelisation schemes can be broadly placed into the two categories
of particle domain decomposed and particle list decomposed.

Particle domain decomposed schemes tend to place particles that are
close in space onto the same process.
%
In this manner, a process may be considered to represent a region of space.
%
Truncations may be implemented by having a process communicate
only with processes representing regions of space within the required
cutoff distance.
%
As a result, the cutoff distance not only reduces the time complexity
of finding the force term for each particle on each time step,
but it also reduces the time complexity of communications
\cite{plimpton1995fast}.
%
This, however, introduces some difficulty to increasing the number
of processes used beyond the point where the space allocated to
each process is smaller than the cutoff distance.

Another suggestion is to have each process approximate, for example,
the charges of all the particles local to it as a single point charge
at the centre of its region of space.
%
Processes then gather the positions and charges of these point charges
created by every other process and incorporate them into the equations
of motion for their local list of particles.
%
This is exemplified by the fast multipole method
\cite{appel1985efficient},
which may be used to produce $\bigO{N}$
calculation times \cite{esselink1992order}.


Particle list decomposed approaches tend to ignore the spatial positions
of the particles.
%
The list of particles is distributed to or across the processes in some manner,
which then cooperate to update the list by distributing the update
operations across the processes.
%
It is therefore not necessary to use any approximations to
implement these schemes.
%
They may, however, be combined in some cases with
approximation schemes to reduce time complexity
\cite{smith1994parallel}.

%
%Q: What force parallelization schemes will we be looking at and why?
Two particle list decomposed schemes will be discussed in detail here and
later implemented and their performance analysed.
%
They are the \replicateddata{} and \systolicloop{} schemes.
%
Two further schemes inspired by these, \sharedandreplicateddata{} and the
\replicatedsystolicloop{}, will also be discussed here and
later be implemented and analysed.



\subsection{Replicated Data}
\label{sec:background:subsec:replicated_data}

%
%Q: What is the replicated data scheme?
In the \replicateddata{} scheme, illustrated in
\FIG{fig:replicated_data_illustration},
the entire system of particles is replicated across all processes.
%
This is useful for the force update, as each particle must have a view of
every other particle to determine its instantaneous force.
%
Given $P$ replicated processes,
each process determines the force for $N/P$ of the particles.
%
The processes then share these newly determined forces with each other
such that each process will have a copy of the system where every
particle has its force set to the correct value for that time step.
%
The processes may then step their system forward in time.
%
In this way, the \replicateddata{} scheme may provide
an $\bigO{N^2/P}$ algorithm.


\begin{figure}
    \begin{center}
    \begin{tikzpicture}[scale=.5]
        % Draw replicated blocks and force areas
        \foreach \n in {0,...,3} {
            \draw [block] (6*\n,0) rectangle (6*\n+4,4);
            \draw [block, redfill] (6*\n,\n) rectangle (6*\n+4,\n+1);
        }
        %
        % Draw arrows
        \foreach \n in {0,...,2} {
            \draw [latex'-latex', thick] (6*\n+4.2,2) -- (6*\n+5.8,2);
        }
        %
        % Draw text
        \foreach \n in {0,...,3} {
            \node at (2 + 6*\n, -1) {\n};
        }
    \end{tikzpicture}
    \end{center}
    %
    \caption{
        An outline of the replicated data scheme.
        Each process, numbered 0-4,
        holds a copy of the entire system of particles,
        represented here in blue.
        Each process is responsible for updating a subset of the
        particles in the system, represented here in red.
        Processes then communicate these changes to each other
        to keep each copy of the system up to date.
    }
    \label{fig:replicated_data_illustration}
\end{figure}


%
%Q: What are the scaling challenges of replicated data?
When communication times are taken into account, however, it becomes
clear that this scheme will not scale as $\bigO{N^2/P}$ indefinitely.
%
An immediate concern is that each process must have the same view of the
system at all times.
%
As a result, all processes must share their updated particles at
every time step and receive updated particles from other processes.
%
This results in a number of system-wide of communications occurring
at every time step,
with $\log{P}$ communications of $N$ particles
yielding a communication time of $\bigO{N \log{P}}$.
%
For large $P$, the $\bigO{N^2/P}$ term will be small compared to the
$\bigO{N\log{P}}$ term, and communications will dominate run time.

As every process has an individual copy of the system of particles,
when large systems are simulated using a large number of cores,
the memory per node of the machine running the simulation
can become a limiting factor to the maximum system size.
%
For a system of particles where each particle uses double
precision floating point (8~B) data to store
a position vector,
a velocity vector,
a force vector and
a scalar mass,
amounting to 80~B per particle, a 32~GB node could hold around $10^8$
particles.
%
However, if the simulation is performed on 10 cores per node,
the machine can now hold a maximum of around $10^7$ particles
as there will be 10 copies of the system on each node.
%
As a result, the maximum possible size of the system is
inversely proportional to
the number of cores used per GB of memory per node.

Another issue arising from this scheme is
that every process must communicate with
every other process at every time step.
%
This puts global synchronisation points into the simulation.
%
Frequent global synchronisation and global data exchange can incur
significant overhead for the calculation.

%
%Q: Why are the scaling challenges of replicated data concerning for exascale?
These drawbacks are particularly important when considering exascale systems
where systems are expected to have large numbers of cores per node
\cite{dongarra2011international}.
%
As memory per node is expected to grow at a much lower rate than
the number of cores per node,
a \replicateddata{} scheme may have difficulties
utilising every core on a node.
%
Even if all the cores in the node are utilised, frequent global synchronisation
points may not scale well to large numbers of processes.

The \replicateddata{} scheme can be seen in use in MD packages such as
AMBER \cite{pearlman1995amber}, CHARMM \cite{brooks1983charmm} and
DL\_POLY \cite{smith1996dlpoly2}.
%
It is worth noting, however, that these packages are rather old,
dating before the year 2000.
%
Nevertheless, they are still actively used, so the communication
patterns present are still worth investigating.




\subsection{Systolic Loop}
\label{sec:background:subsec:systolic_loop}

%
%Q: What is the systolic loop scheme?
In the \systolicloop{} scheme, the system of particles is
split up and distributed among all the processes.
%
Force updates are performed by
passing packets of particles around a ring in pulses.
%
This is illustrated in \FIG{fig:systolic_loop_illustration}.

\begin{figure}
    \begin{center}
    \begin{tikzpicture}
        %
        % Draw bounding boxes
        \draw [draw, rounded corners] (-0.5,0) rectangle +(2, 1);
        %
        \draw [draw, rounded corners] (2,1.5) rectangle +(1, 2);
        \draw [draw, rounded corners] (4,1.5) rectangle +(1, 2);
        %
        \draw [draw, rounded corners] (5.5,0) rectangle +(2, 1);
        %
        \draw [draw, rounded corners] (4,-0.5) rectangle +(1, -2);
        \draw [draw, rounded corners] (2,-0.5) rectangle +(1, -2);
        %
        % Draw outer ring
        \foreach \xy in {
            {(-0.5,0)},
            {(2,2.5)}, {(4,2.5)},
            {(6.5,0)},
            {(4,-2.5)}, {(2,-2.5)}
        } {
            \draw [block, bluefill] {\xy+(0.1,0.1)} rectangle +(0.9, 0.9);
        }
        %
        % Draw inner ring
        \foreach \xy in {
            {(0.5,0)},
            {(2,1.5)}, {(4,1.5)},
            {(5.5,0)},
            {(4,-1.5)}, {(2,-1.5)}
        } {
            \draw [block, redfill] {\xy+(0.1,0.1)} rectangle +(0.9, 0.9);
        }
        %
        % Draw circle
        \newcommand{\off}{0.1}
        \path [line] (1, 0.5+\off) |- +(1.5, 1.5-\off);
        %
        \path [line] (2.5+\off, 2) -- +(2-\off, 0);
        \path [line] (4.5+\off, 2) -| +(1.5-\off, -1.5);
        %
        \path [line] (6, 0.5-\off) |- +(-1.5, -1.5+\off);
        %
        \path [line] (4.5-\off, -1) -- +(-2+\off, 0);
        \path [line] (2.5-\off, -1) -| +(-1.5+\off, +1.5);
        %
        % Draw labels
        \foreach \xy/\n in {
            {(-1,0.5)}/0,
            {(2.5,4)}/1, {(4.5,4)}/2,
            {(8,0.5)}/3,
            {(4.5,-3)}/4, {(2.5,-3)}/5
        } {
            \node at \xy {\n};
        }
    \end{tikzpicture}
    \end{center}
    %
    \caption{
        A representation of the systolic loop model.
        Each process, numbered 0-5, holds a subset of the entire
        system of particles, represented in blue.
        Processes pass packets of particles, represented in red,
        around the loop.
        When a process receives a packet, it partially updates its local
        system of particles.
        When it has received every packet, it will have fully updated
        its local particles.
    }
    \label{fig:systolic_loop_illustration}
\end{figure}

Initially, a process will make a copy of its list of particles,
and then use that to perform a partial force update on its local list.
%
It will then pass the copied list to the ``right'' process and
receive a new list from the ``left'' process and use that to perform
another partial force update on its local list.
%
These pulses continue until a process receives its own system again from
the left process, signifying that it has at some stage received
a copy of every list on every process.

At this point, it should have added together the force contributions from
every particle in the system for each particles in its local list.
%
From here, a process may update the position and velocity of these local
particles, thereby stepping the system forward.
%
As comparing lists of particles on each pulse is an $\bigO{(N/P)^2}$ operation
and each time step requires $P$ pulses, the calculation is expected to take
$\bigO{N^2/P}$ time.


%
%Q: What are the scaling challenges of systolic loop?
As the system is distributed over a number of processes, this allows
the scheme to work with very large systems.
%
Taking the systems and figures from the previous section,
rather than the simulation being limited to $10^7$ particles for the system,
it is instead limited to $10^7$ particles per core per 32~GB node.
%
There is an upper limit on particles per core, but no
upper limit on the size system being studied as more cores can be introduced
to reduce the number of particles per core used by the simulation.

Another benefit is that
each process communicates with only two other neighbouring processes
and sends and receives only $N/P$ particles.
%
By this mechanism, volume of data
being communicated across the machine is limited
and there are no strict global synchronisation points.
%
Communications and calculations may be overlapped during a systolic pulse.

When factoring in the number of systolic pulses per time step,
it becomes clear that the $\bigO{N^2/P}$ scaling will not last indefinitely.
%
As a result of communication latency, a communication term of
$\bigO{P}$ exists.
%
As the calculation scales inversely with the number of processes
and the communication linearly, it is clear that communication time will
dominate over calculation much sooner here than
in the \replicateddata{} scheme.

%
%Q: Why are the scaling challenges of systolic loop concerning for exascale?
As increasing the number of processes to a large value
can quickly result in calculations having significant communication overhead,
this raises an immediate concern for exascale systems.
%
However, the ability to partition data across processes is valuable.


The \systolicloop{} pattern can be seen, to a certain extent, in the
GROMACS \cite{berendsen1995gromacs} package which uses a ring
topology for particle list decomposition.
%
However, this was removed in GROMACS 4 \cite{hess2008gromacs}.



\subsection{Shared And Replicated Data}

The \sharedandreplicateddata{} scheme is intended to be an improvement
upon the \replicateddata{} scheme.
It is implemented in exactly the same
manner except the force update loop is further
parallelised using \openmp{} directives, taking advantage of shared
memory between cores.

The primary motivation for this is to show that mixed mode MPI and \openmp{}
parallelism performs just as well as MPI only parallelism for a
\replicateddata{} and is quite easily implemented.
%
The importance of this is that the maximum system size that can
be run on a machine
can be increased in proportion to the number of \openmp{} threads
created per MPI process with a relatively small change to the code.

In this scheme, the system of particles is replicated across
$P_{MPI}$ MPI processes.
%
Each MPI process then spawns $P_{OMP}$ threads, resulting in a total
of $P_{MPI}\times{}P_{OMP} = P$ threads.
%
This provides a force calculation time of $N^2/P$.

As there are $P_{MPI} = P/P_{OMP}$ processes, communications are
reduced from $\log{P}$ as in the \replicateddata{} case to
\begin{equation}
    \log{P_{MPI}} = \log{\frac{P}{P_{OMP}}} = \log{P} - \log{P_{OMP}}
\end{equation}
%
It is clear that the introduction of \openmp{} threads should
bring a slight reduction to computation time, albeit a limited one.
%
To achieve a reasonable improvement in communications from this method,
a large number of \openmp{} threads per MPI process would be needed.

Where this scheme provides the most benefit is in data storage considerations.
%
Where in the \replicateddata{} case, the maximum system size was a function
of the number of cores per node, here it is the number of MPI processes
per node.
%
By increasing the number of \openmp{} threads per MPI process,
the maximum system size possible is increased
in proportion with $P_{OMP}$.


Where there were concerns for the \replicateddata{} scheme at exascale
due to cores per node increasing faster than memory per node,
this scheme has no such qualms.
%
Where more cores per node are introduced,
more \openmp{} threads per MPI process
may be introduced with no increase to
the memory footprint of the program.
%
However, this approach does still suffer from global synchronisation issues.



\subsection{Replicated Systolic Loop}

%
%Q: What is the replicated systolic loop scheme?
The \replicatedsystolicloop{} scheme is intended to directly improve directly
upon the \systolicloop{} scheme.
%
It takes some inspiration from the \replicateddata{} scheme's approach
to partitioning workloads.
%
However, instead of assigning subsets of the particle list to replicas
to update, subsets of systolic pulses are assigned.

Given $P$ processes, they are arranged in a Cartesian topology of
approximate size $\sqrt{P} \times{} \sqrt{P}$.
%
A systolic loop is then created with the processes
$(0,0) \rightarrow{} (\sqrt{P}, 0)$
resulting in a systolic loop with $\sqrt{P}$ systolic elements.
%
The system of particles is partitioned across these systolic elements.
%
This systolic loop is then replicated on processes
$(0,1) \rightarrow{} (\sqrt{P}, 1)$.
%
It is similarly replicated on processes $(0,y) \rightarrow{} (\sqrt{P},y)$.

The topology may not necessarily be split perfectly into a
$\sqrt{P} \times{} \sqrt{P}$ grid.
%
As such, it will instead be said that the processes are split into a
$S \times{} R$ grid.
%
Each replica systolic loop will have $S$ systolic elements,
and there will be $R$ replica loops.


In this manner, by using $P$ processes, each process should be holding
$N/S$ particles.
%
Similar to the \systolicloop{} case, the maximum system size is a function
of the number of cores in the machine, rather than the cores per node.
%
However, unlike the \systolicloop{} case, particles per core grows as
$N/S$ rather than $N/P$, which means a greater number of processes
may be needed to partition a large system enough to fits in
a node's memory.

As $S \times{} R = P$,
if the system of particles must be partitioned into
smaller chunks per core,
fewer replicas may be made in favour of more systolic elements
per replica loop.
%
This will result in a lower particle per core count.
%
However, this will have an effect on communication times.


A time step for a force comparison proceeds as follows:
each replica is assigned $S/R$ of the $S$ systolic pulses to
perform.
%
Replica $x$, for example, will be assigned the pulses
$xS/R \rightarrow{} (x+1)S/R$.
%
This is performed by having each element of loop $x$ perform an initial
swap, sending its local list of particles to
the element $xS/R$ to the ``right'',
and receiving a list from the element $xS/R$ to the ``left'',
and then performing $S/R$ pulses.

When each replica performs its $S/R$ pulses, each element
should have only a partial result, having performed only a subset
of the pulses.
%
Equivalent systolic elements are defined as elements whose
process coordinates have the same $y$ value but different $x$ values.
%
Each equivalent systolic element should have performed a different
subset of the necessary pulses, resulting in each equivalent element
having a different piece of the overall result.
%
Equivalent systolic elements communicate to
reduce these partial results together.
%
The resulting reduction should be the result of a full set of systolic pulses
for that systolic element.



The computation time for a list comparison during a single pulse
should be $\bigO{(N/S)^2}$.
%
For $S/R$ pulses, this should result in a time $\bigO{N^2/(SR)} = \bigO{N^2/P}$
which is the familiar \systolicloop{} calculation time.
%
It can be seen that this scheme requires $S \ge{} R$.
%
Similar to the \systolicloop{} scheme, it also requires $N \ge{} S$.
%
As a result, assuming $S = R = \sqrt{P}$, this scheme can be run
with $P = N^2$, which is a large improvement over the \systolicloop{}
maximum of $P = N$.



Communication considerations suggest the $\bigO{N^2/P}$ scaling
will likely not continue until $P = N^2$.
%
Assuming each process is holding $N/S$ particles, then $S/R$
systolic pulses of $N/S$ particles with a constant latency $l$
should require a communication time of $\bigO{N/R + S/R}$.
%
Where communication latency in the \systolicloop{} scheme produced
a $\bigO{P}$ scaling term, if the number of replicas used is
similar to the number of systolic elements, this $\bigO{P}$ term
becomes $\bigO{S/R} = \bigO{1}$ here.

Finally, the reduction across equivalent systolic elements of
$N/S$ particles with a constant latency results in a
$\bigO{N\log{R}/S + \log{R}}$ term.
%
This is particularly interesting, as the $N\log{R}/S$ term can
decrease with larger $P$ by keeping $S \sim{} R$,
as $\log{x}/x$ is a decreasing function.
%
The communication time at large $P$ can therefore be independent
of the number of particles in the system.

This scheme is well adapted to the low memory per core environment
envisaged by exascale computing as memory per core can be reduced
simply be adding more cores to the simulation.
%
Similar to the systolic loop, some hard synchronisation points have been
removed.
%
Calculations and communications may be overlapped when performing
systolic pulses.
%
This method does, however, include an \mpiallreduce{} call, which represents
a synchronisation point between disjoint sets of $\sqrt{P}$ processes,
each process in a different replica systolic loop.
%
So while the interplay of systolic pulses and equivalent element reductions
suggest some global synchronicity, there are no hard
global synchronisation points.
