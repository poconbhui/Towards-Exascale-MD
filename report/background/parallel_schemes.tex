\section{Parallel Schemes}

%
%Q: Can we parallelize the force calculation?
The \velocityverlet{} scheme with a \twobody{} potential
immediately provides an algorithm for
performing MD simulations of long ranged forces in isolated systems.
%
This dissertation will explore parallel implementations of this algorithm.

%
%Q: What force parallelization schemes exist?
Parallelisation schemes can be broadly placed into the two categories
of particle domain sensitive and particle domain insensitive.

The domain decomposed approach is a domain sensitive scheme
that places particles that are
close in space onto processors that are close on the machine running
the simulation.
%
In this manner, a process may be considered to represent a region of space.
%
Truncations may be implemented by having a process communicate
only with processes representing regions of space within the required
cutoff distance.
%
As a result, the cutoff distance not only reduces the time complexity
of finding the force term for each particle on each time step,
but it also reduces the time complexity of communications.
%
This, however, introduces some difficulty to increasing the number
of processors beyond the point where the space allocated to
each process is smaller than the cutoff distance.

Another suggestion is to have each process approximate, for example,
the charges of all the particles local to it as a single point charge
at the center of its region of space.
%
Processes then gather the positions and charges of these point charges
created by every other process and incorporate them into the equations
of motion for their local list of particles.
%
This is exemplified by the fast multipole method, which may be used
to produce $\bigO{N}$ calculation times.


Particle domain insensitive approaches tend to ignore the spatial positions
of the particles.
%
The list of particles is distributed to or across  the processes in some manner,
which then cooperate to update a section of the list and update
the entire list as a result.
%
It is therefore not necessary to use any approximations to
implement these schemes.
%
They may, however, be combined with approximation schemes to reduce
time complexity.

%
%Q: What force parallelization schemes will we be looking at and why?
Two domain insensitive schemes will be discussed in detail here and
later implemented and their performance analysed.
%
They are the \replicateddata{} and \systolicloop{} schemes.
%
Two further schemes, \sharedandreplicateddata{} and the
\replicatedsystolicloop{}, will later be implemented and analysed.
%
These have been chosen for the relative simplicity of implementation
and to avoid the need to consider the correctness of any
approxmations that might be implemented as part of a parallel scheme.



\subsection{Replicated Data}
\label{sec:background:subsec:replicated_data}

%
%Q: What is the replicated data scheme?
In the \replicateddata{} scheme, illustrated in
\FIG{fig:replicated_data_illustration},
the entire system of particles is replicated across all processors.
%
This is useful for the force update, as each particle must have a view of
every other particle to determine its instantaneous force.
%
Given $P$ replicated processes,
each process determines the force for $1/P$ of the particles.
%
The processes then share these newly determined forces with each other
such that each processor will have a copy of the system where every
particle has its force set to the correct value for that time step.
%
The processes may then step their system forward in time.
%
In this way, the \replicateddata{} scheme may provide
an $\bigO{N^2/P}$ algorithm.


\begin{figure}[!ht]
    \begin{center}
    \begin{tikzpicture}[scale=.5]
        % Draw replicated blocks and force areas
        \foreach \n in {0,...,3} {
            \draw [block] (6*\n,0) rectangle (6*\n+4,4);
            \draw [block, redfill] (6*\n,\n) rectangle (6*\n+4,\n+1);
        }
        %
        % Draw arrows
        \foreach \n in {0,...,2} {
            \draw [latex'-latex', thick] (6*\n+4.2,2) -- (6*\n+5.8,2);
        }
        %
        % Draw text
        \foreach \n in {0,...,3} {
            \node at (2 + 6*\n, -1) {\n};
        }
    \end{tikzpicture}
    \end{center}
    %
    \caption{
        An outline of the replicated data scheme.
        Each process, numbered 0-4,
        holds a copy of the entire system of particles,
        represented here in blue.
        Each process is responsible for updating a subset of the
        particles in the system, represented here in red.
        Processes then communicate these changes to each other
        to keep each copy of the system up to date.
    }
    \label{fig:replicated_data_illustration}
\end{figure}


%
%Q: What are the scaling challenges of replicated data?
When communication times are taken into account, however, it becomes
clear that this scheme will not scale as $\bigO{N^2/P}$ indefinitely.
%
An immediate concern is that each process must have the same view of the
system at all times.
%
As a result, all processes must share their updated particles at
every time step and receive updated particles from other processes.
%
This results in a number of system-wide of communications occurring
at every time step,
with $\log{P}$ communications of $N$ particles
yielding a communication time of $\bigO{N \log{P}}$.
%
For large $P$, the $\bigO{N^2/P}$ term will be small compared to the
$\bigO{N\log{P}}$ term, and communications will dominate run time.

As every process has an individual copy of the system of particles,
when large systems are simulated using a large number of cores,
the memory per node of the machine running the simulation
can become a limiting factor to the maximum system size.
%
For a system of particles where each particle uses double
precision floating point (8~B) data to store
a position vector,
a velocity vector,
a force vector and
a scalar mass,
amounting to 80~B per particle, a 32~GB node could hold around $10^8$
particles.
%
However, if the simulation is performed on 10 cores per node,
the machine can now hold a maximum of around $10^7$ particles
as there will be 10 copies of the system on each node.
%
As a result, the maximum possible size of the system is
inversely proportional to
the number of cores used per GB of memory per node.

Another issue arising from this scheme is
that every process must communicate with
every other process at every time step.
%
This puts global synchronisation points into the simulation.
%
Frequent global synchronisation and global data exchange can incur
significant overhead for the calculation.

%
%Q: Why are the scaling challenges of replicated data concerning for exascale?
These drawbacks are particularly important when considering exascale systems
where systems are expected to have large numbers of cores per node.
%
As memory per node is expected to grow at a much lower rate than
the number of cores per node,
a \replicateddata{} scheme may have difficulties
utilising every core on a node.
%
Indeed, even with current machines, nodes are often requested and only a
subset of cores used due to memory per core considerations in applications.
%
Even if all the cores in the node are utilised, frequent global synchronisation
points may not scale well to large numbers of processes.



\subsection{Systolic Loop}
\label{sec:background:subsec:systolic_loop}

%
%Q: What is the systolic loop scheme?
In the \systolicloop{} scheme, the system of particles is
split up and distributed among all the processes.
%
Force updates are performed by
passing packets of particles around a ring in pulses.
%
This is illustrated in \FIG{fig:systolic_loop_illustration}.

\begin{figure}[!ht]
    \begin{center}
    \begin{tikzpicture}
        %
        % Draw bounding boxes
        \draw [draw, rounded corners] (-0.5,0) rectangle +(2, 1);
        %
        \draw [draw, rounded corners] (2,1.5) rectangle +(1, 2);
        \draw [draw, rounded corners] (4,1.5) rectangle +(1, 2);
        %
        \draw [draw, rounded corners] (5.5,0) rectangle +(2, 1);
        %
        \draw [draw, rounded corners] (4,-0.5) rectangle +(1, -2);
        \draw [draw, rounded corners] (2,-0.5) rectangle +(1, -2);
        %
        % Draw outer ring
        \foreach \xy in {
            {(-0.5,0)},
            {(2,2.5)}, {(4,2.5)},
            {(6.5,0)},
            {(4,-2.5)}, {(2,-2.5)}
        } {
            \draw [block, bluefill] {\xy+(0.1,0.1)} rectangle +(0.9, 0.9);
        }
        %
        % Draw inner ring
        \foreach \xy in {
            {(0.5,0)},
            {(2,1.5)}, {(4,1.5)},
            {(5.5,0)},
            {(4,-1.5)}, {(2,-1.5)}
        } {
            \draw [block, redfill] {\xy+(0.1,0.1)} rectangle +(0.9, 0.9);
        }
        %
        % Draw circle
        \newcommand{\off}{0.1}
        \path [line] (1, 0.5+\off) |- +(1.5, 1.5-\off);
        %
        \path [line] (2.5+\off, 2) -- +(2-\off, 0);
        \path [line] (4.5+\off, 2) -| +(1.5-\off, -1.5);
        %
        \path [line] (6, 0.5-\off) |- +(-1.5, -1.5+\off);
        %
        \path [line] (4.5-\off, -1) -- +(-2+\off, 0);
        \path [line] (2.5-\off, -1) -| +(-1.5+\off, +1.5);
        %
        % Draw labels
        \foreach \xy/\n in {
            {(-1,0.5)}/0,
            {(2.5,4)}/1, {(4.5,4)}/2,
            {(8,0.5)}/3,
            {(4.5,-3)}/4, {(2.5,-3)}/5
        } {
            \node at \xy {\n};
        }
    \end{tikzpicture}
    \end{center}
    %
    \caption{
        A representation of the systolic loop model.
        Each process, numbered 0-5, holds a subset of the entire
        system of particles, represented in blue.
        Processes pass packets of particles, represented in red,
        around the loop.
        When a process receives a packet, it partially updates its local
        system of particles.
        When it has received every packet, it will have fully updated
        its local particles.
    }
    \label{fig:systolic_loop_illustration}
\end{figure}

Initially, a process will make a copy of its list of particles,
and then use that to perform a partial force update on its local list.
%
It will then pass the copied list to the ``right'' process and
receive a new list from the ``left'' process and use that to perform
another partial force update on its local list.
%
These pulses continue until a process receives its own system again from
the left process, signifying that it has at some stage received
a copy of every list on every process.
%
At this point, it should have added the force contributions from
every particle in the system to the particles in its local list.
%
From here, a process may update the position and velocity of these local
particles, thereby stepping the system forward.
%
As comparing lists of particles on each pulse is an $\bigO{(N/P)^2}$ operation
and each time step requires $P$ pulses, the calculation is expected to take
$\bigO{N^2/P}$ time.


%
%Q: What are the scaling challenges of systolic loop?
As the system is distributed over a number of processes, this allows
the scheme to work with very large systems.
%
Taking the systems and figures from the previous section,
rather than the simulation being limited to $10^7$ particles for the system,
it is instead limited to $10^7$ particles per core per 32~GB node.
%
There is an upper limit on particles per core, but no
upper limit on the size system being studied as more cores can be introduced
to reduce the number of particles per core used by the simulation.

Another benefit is that
communications between processes are more limited than
the replicated case.
%
Here, each process communicates with only two other neighbouring processes
and sends and receives only $N/P$ particles.
%
By this mechanism, volume of data
being communicated across the machine is limited
and and there are no strict global synchronisation points.

When factoring in the number of systolic pulses per time step,
it becomes clear that the $\bigO{N^2/P}$ scaling will not last indefinitely.
%
As a result of communication latency, a communication term of
$\bigO{P}$ exists.
%
As the calculation scales inversely with the number of processes
and the communication linearly, it is clear that communication time will
dominate over calculation much sooner here than
in the \replicateddata{} scheme.

%
%Q: Why are the scaling challenges of systolic loop concerning for exascale?
As increasing the number of processes to a large value
can quickly result in calculations having significant communication overhead,
this raises an immediate concern for exascale systems.
